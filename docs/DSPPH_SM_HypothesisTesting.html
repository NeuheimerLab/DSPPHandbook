<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Skills Portfolio Program - Statistical Modelling: Hypothesis Testing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./data_skills_logo_2.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Data Skills Portfolio Program</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./intro.html" rel="" target="">
 <span class="menu-text">What is the DSP Program?</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./handbookIntro.html" rel="" target="">
 <span class="menu-text">The DSP Program Handbook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./feedback.html" rel="" target="">
 <span class="menu-text">Feedback</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-hypothesis-testing" id="toc-what-is-hypothesis-testing" class="nav-link active" data-scroll-target="#what-is-hypothesis-testing"><span class="header-section-number">1</span> What is hypothesis testing?</a></li>
  <li><a href="#sec_PValue" id="toc-sec_PValue" class="nav-link" data-scroll-target="#sec_PValue"><span class="header-section-number">2</span> Hypothesis testing using P-values</a>
  <ul class="collapse">
  <li><a href="#what-is-a-p-value" id="toc-what-is-a-p-value" class="nav-link" data-scroll-target="#what-is-a-p-value"><span class="header-section-number">2.1</span> What is a P-value</a></li>
  <li><a href="#limitations-of-p-values" id="toc-limitations-of-p-values" class="nav-link" data-scroll-target="#limitations-of-p-values"><span class="header-section-number">2.2</span> Limitations of P-values</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing-using-model-selection" id="toc-hypothesis-testing-using-model-selection" class="nav-link" data-scroll-target="#hypothesis-testing-using-model-selection"><span class="header-section-number">3</span> Hypothesis testing using model selection</a>
  <ul class="collapse">
  <li><a href="#what-is-model-selection" id="toc-what-is-model-selection" class="nav-link" data-scroll-target="#what-is-model-selection"><span class="header-section-number">3.1</span> What is model selection</a></li>
  <li><a href="#how-do-you-use-hypothesis-testing-for-model-selection" id="toc-how-do-you-use-hypothesis-testing-for-model-selection" class="nav-link" data-scroll-target="#how-do-you-use-hypothesis-testing-for-model-selection"><span class="header-section-number">3.2</span> How do you use hypothesis testing for model selection</a>
  <ul class="collapse">
  <li><a href="#form-your-candidate-model-set" id="toc-form-your-candidate-model-set" class="nav-link" data-scroll-target="#form-your-candidate-model-set">Form your candidate model set</a></li>
  <li><a href="#fit-and-rank-models-in-your-candidate-model-set" id="toc-fit-and-rank-models-in-your-candidate-model-set" class="nav-link" data-scroll-target="#fit-and-rank-models-in-your-candidate-model-set">Fit and rank models in your candidate model set</a></li>
  <li><a href="#choose-your-best-specified-models" id="toc-choose-your-best-specified-models" class="nav-link" data-scroll-target="#choose-your-best-specified-models">Choose your best-specified model(s)</a></li>
  <li><a href="#what-does-your-best-specified-models-say-about-your-hypothesis" id="toc-what-does-your-best-specified-models-say-about-your-hypothesis" class="nav-link" data-scroll-target="#what-does-your-best-specified-models-say-about-your-hypothesis">What does your best-specified model(s) say about your hypothesis?</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Statistical Modelling: Hypothesis Testing</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In this section you will learn:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><p>how statistical models can be used to test your hypothesis by judging the evidence for your model</p></li>
<li><p>about methods to judge the evidence for your model</p></li>
<li><p>to use the model selection method to judge the evidence for your model and test your hypothesis</p></li>
</ul>
</div>
</div>
</div>
<section id="what-is-hypothesis-testing" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What is hypothesis testing?</h1>
<p>Once you have validated your starting model, you can finally arrive at the reason for your statistical journey: testing your model to see what evidence there is for your research hypothesis. <strong>Finally - the science!</strong> By testing your model, you will find out what inference<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> can be made from your modelled effects.</p>
<p>Recall that fitting your starting model meant that you estimated the parameter for each coefficient associated with your predictors. These are your modelled effects.</p>
<p>Let’s take an example where we want to explain variability in change in weight (<code>WtChange</code>, g) and believe it is due to prey density (<code>Prey</code>, <span class="math inline">\(num \cdot m^{-3}\)</span>). Here, we have <code>Prey</code> as a numeric<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> predictor.</p>
<p>We will test the hypothesis <code>WtChange ~ Prey</code> by first fitting a model with a normal error distribution assumption and a linear shape assumption<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>If our research hypothesis is</p>
<p><em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span>,</p>
<p>our starting model will fit:</p>
<p><em>WtChange</em> <span class="math inline">\(\sim \beta_1\cdot Prey + \beta_0 + error\)</span></p>
<p>where the coefficients are the slope (<span class="math inline">\(\beta_1\)</span>) and intercept (<span class="math inline">\(\beta_0\)</span>), and the error is based on a normal error distribution.</p>
<p>When you test your hypothesis, you are focusing on the estimates of your model coefficients<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and whether or not these estimates are different than zero.</p>
<p>For example, if <span class="math inline">\(\beta_1 \approx 0\)</span>, the effect of <code>Prey</code> on <code>WtChange</code> would be zero, the <code>Prey</code> predictor would be removed from the model, and you would conclude that there is little evidence that <code>Prey</code> explains variability in <code>WtChange</code>.</p>
<p>So, to test your research hypothesis, you need to determine if each of the coefficients (effects) are significantly different than zero.</p>
<p>This can be done using a number of methods<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Two are:</p>
<ul>
<li><p>the P-value<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> method: The first method estimates the probability (P-value) that your coefficient (e.g.&nbsp;<span class="math inline">\(\beta_1\)</span>) would be estimated at the value it is even though the “real” value of your coefficient is 0. <a href="#sec_PValue">More on P-values below</a>.</p></li>
<li><p>the model selection method: The second method involves comparing models with different predictor combinations (model selection). Here, you will consider the evidence for models with and without each of your predictors to determine the evidence that each coefficient is different than zero.</p></li>
</ul>
<p>The method that you can use to test your hypothesis can depend on the design of the study you use to explore your research hypothesis - i.e.&nbsp;experimental vs.&nbsp;observational<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> studies<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. With experimental studies, where you are able to control (to a good extent) the collinearity among your predictors<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, you can use either the P-value or model selection methods. For observational studies, the model selection method is a more robust way to test your hypothesis. For this reason<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, this handbook will primarily focus on the model selection method but let us first discuss the P-value method (what it is, and its limitations).</p>
</section>
<section id="sec_PValue" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Hypothesis testing using P-values</h1>
<section id="what-is-a-p-value" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="what-is-a-p-value"><span class="header-section-number">2.1</span> What is a P-value</h2>
<p>The P-value is used for null-hypothesis significance testing (NHST) (<span class="citation" data-cites="Muff2022">Muff et al. (<a href="#ref-Muff2022" role="doc-biblioref">2022</a>)</span>). The “P” in P-value stands for probability - the probability of observing an outcome given that the null hypothesis is true (<span class="citation" data-cites="Muff2022">Muff et al. (<a href="#ref-Muff2022" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="Popovic2024">Popovic et al. (<a href="#ref-Popovic2024" role="doc-biblioref">2024</a>)</span>). Remember that the null-hypothesis assumes that the tested effect is zero. In the case of hypothesis testing, the null hypothesis test assumes a coefficient describing the effect of a predictor on your response is zero.</p>
<p>In the case of hypothesis testing, the null hypothesis you are testing against is that a predictor’s coefficient is zero. So, the P-value associated with the hypothesis testing tells you the probability of getting a coefficient at least as big as your value even though the coefficient is in fact zero.</p>
<p>When the P-value is very low, we say that there is evidence that the coefficient is not zero, i.e.&nbsp;evidence that your predictor has an effect on your response. By convention, we say a P-value is low if P &lt; 0.05; meaning that the evidence comes with a 5% probability that the coefficient is actually zero. The research community has decided that less than a 5% probability is a level of uncertainty with which we are comfortable.</p>
<p>First, let’s describe how this works in general, and then look at an example:</p>
<p>To determine a P-value associated with a model coefficient, the null-hypothesis testing estimates something called a test statistic based on the coefficient’s estimate and the error around it. This test statistic is assumed to come from a certain data distribution (the exact distribution will vary based on your model structure)</p>
<p>Let us look at an example using our model fit to the hypothesis <code>WtChange ~ Prey + 1</code>. By using <code>summary()</code> on our model, we get</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(validMod) <span class="co"># look at our validated starting model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = WtChange ~ Prey, family = gaussian(link = "identity"), 
    data = myDat)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.996353   0.158745  -44.07   &lt;2e-16 ***
Prey         0.079912   0.002557   31.25   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 0.1414749)

    Null deviance: 147.8242  on 69  degrees of freedom
Residual deviance:   9.6203  on 68  degrees of freedom
AIC: 65.728

Number of Fisher Scoring iterations: 2</code></pre>
</div>
</div>
<p>The coefficients table shows us that the Intercept was estimated as -7 ± 0.16 g and the slope associated with <code>Prey</code><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> is 0.08 ± 0.0026 <span class="math inline">\(g \cdot m^{3}\cdot num^{-1}\)</span>.</p>
<p>For each coefficient, you can see t-statistic (called <code>t value</code> in the table) and P-value (called <code>Pr(&gt;|t\)</code> in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g.&nbsp;for the intercept in the example, -7/0.16 = -44.07). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is actually zero. This probability is the P-value. When P-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, and that the predictor associated with the coefficient can be included in our model (i.e.&nbsp;the predictor is explaining a significant amount of our response variability).</p>
<p>Note that some null-hypothesis tests using P-values will use different test statistics. For example, your model summary will show a z-statistic instead of a t-statistic for models that only measure the mean of the coefficient value (vs.&nbsp;more than one parameter, e.g.&nbsp;mean and the scale parameter).</p>
<p><strong>How to estimate P-values for your model</strong></p>
<p>The output from the <code>summary()</code> function quickly becomes limiting when you have more than one predictor. Instead, you can use the <code>anova()</code> function to estimate the P-values associated with each model term.</p>
<p>Here is an example for our model testing <em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(validMod, <span class="co"># model object</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">test =</span> <span class="st">"F"</span>) <span class="co"># type of null hypothesis test to perform</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model: gaussian, link: identity

Response: WtChange

Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev      F    Pr(&gt;F)    
NULL                    69     147.82                     
Prey  1    138.2        68       9.62 976.88 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Note here that you need to indicate what type of null hypothesis testing you want:</p>
<ul>
<li><p>use the F-test for error distribution assumptions like normal (gaussian) or Gamma (i.e.&nbsp;distributions where the scale parameter is estimated)</p></li>
<li><p>use the Chi-square test for error distribution assumptions like poisson or binomial (i.e.&nbsp;distributions where the scale parameter is fixed)</p></li>
</ul>
<p>The result is a table where each predictor has a row to report the results of the null hypothesis test. Here we see that there is strong evidence the coefficient associated with <code>Prey</code> is not zero (P &lt; <span class="math inline">\(2.2 \cdot 10^{-16}\)</span>).</p>
<p>Two more notes about using P-values:</p>
<ol type="1">
<li><p>note in the table above that it says “Terms added sequentially (first to last)”. This indicates that the coefficients of the predictors are tested by adding each predictor one at a time to the model, estimating the coefficient associated with the predictor, and testing the null hypothesis that the coefficient is not different than zero. <strong>This process is problematic when you have even a moderate amount of predictor collinearity.</strong> This is a big reason to prefer the model selection method of hypothesis testing that we outline below.</p></li>
<li><p>Because of the issues interpreting P-values, it is better to talk about what P-values tell you about the evidence for your hypothesis, rather than a strict idea of rejecting or not your hypothesis. Here is an illustration of how to interpret your P-values<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>:</p></li>
</ol>
<p><img src="./MuffPValues.png" align="center" width="400px"></p>
<p>(from <span class="citation" data-cites="Muff2022">Muff et al. (<a href="#ref-Muff2022" role="doc-biblioref">2022</a>)</span>)</p>
</section>
<section id="limitations-of-p-values" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="limitations-of-p-values"><span class="header-section-number">2.2</span> Limitations of P-values</h2>
<p>As mentioned in the previous section, problems with the P-value method of testing your research hypothesis comes when you have more than one predictor in your hypothesis. Correlation among your predictors^<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> means that it is difficult to trust your coefficient estimates. This means that you can not use the P-values as a way to determine which coefficients are significantly different than zero when you have correlated predictors. Said another way, your assessment of whether a predictor is useful in your model will be uncertain if you have correlated predictors. And correlated predictors are very common. For this reason, we will be hypothesis testing using model selection for the remainder of the handbook.</p>
</section>
</section>
<section id="hypothesis-testing-using-model-selection" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Hypothesis testing using model selection</h1>
<p>An alternative method of testing your hypothesis is through model selection. This method is more robust to issues like predictor collinearity and can be generally applied regardless of the structure of your experiment or model.</p>
<section id="what-is-model-selection" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="what-is-model-selection"><span class="header-section-number">3.1</span> What is model selection</h2>
<p>Compare the following two models:</p>
<ol type="1">
<li><p><em>WtChange</em> <span class="math inline">\(\sim \beta_1\cdot Prey + \beta_0 + error\)</span></p></li>
<li><p><em>WtChange</em> <span class="math inline">\(\sim \beta_0 + error\)</span></p></li>
</ol>
<p>Note that model 2 can be made by making the coefficient of <code>Prey</code> (<span class="math inline">\(\beta_1\)</span>) equal to 0 (i.e.&nbsp;if the effect of <code>Prey</code> on <code>WtChange</code> was zero). If you determined which of these two models better fits your data, you will know if <span class="math inline">\(\beta_1\)</span> is likely to be 0 and, thus, whether or not you have evidence that <code>Prey</code> can explain variation in <code>WtChange</code>.</p>
</section>
<section id="how-do-you-use-hypothesis-testing-for-model-selection" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="how-do-you-use-hypothesis-testing-for-model-selection"><span class="header-section-number">3.2</span> How do you use hypothesis testing for model selection</h2>
<p>The steps involved in testing your hypothesis using model selection is</p>
<ul>
<li><p>form your candidate model set</p></li>
<li><p>fit and rank models in your candidate model set</p></li>
<li><p>choose your best-specified model(s)</p></li>
</ul>
<p>Let’s walk through these now.</p>
<section id="form-your-candidate-model-set" class="level3">
<h3 class="anchored" data-anchor-id="form-your-candidate-model-set">Form your candidate model set</h3>
<p>Your candidate model set contains models with all possible predictor combinations<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>. So the candidate model set for <em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span> is:</p>
<p><em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span></p>
<p><em>WtChange</em> <span class="math inline">\(\sim 1\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Another example">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Another example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here is another example:</p>
<p>if your hypothesis is</p>
<p><span class="math inline">\(Resp \sim Pred1 + Pred2 + Pred1:Pred2 + 1\)</span></p>
<p>your candidate model set is</p>
<p><span class="math inline">\(Resp \sim Pred1 + Pred2 + Pred1:Pred2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Pred1 + Pred2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Pred1 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Pred2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim 1\)</span></p>
<p>Note that the more predictors you have in your model, the bigger your candidate model set.</p>
</div>
</div>
</div>
<p>Hopefully you are starting to see that the difference among models in the candidate model set can be described by setting the coefficient associated with a particular predictor to zero. In this way, fitting and comparing the models in your candidate model set is a way of assessing the evidence for your hypothesis. This method is more robust to issues like predictor collinearity because you are assessing the evidence for a predictor’s effect on your response when each predictor is in a model alone and when it is in a model with other predictors.</p>
<p>One last note about your candidate model set: you must remember the biology when you form your candidate model set. There may be a biological reason why a certain model must not be included in your candidate model set (i.e.&nbsp;a model that defies biological logic). These should be excluded from your candidate model set. (<span class="citation" data-cites="Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference">Burnham and Anderson (<a href="#ref-Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference" role="doc-biblioref">2002</a>)</span>).</p>
</section>
<section id="fit-and-rank-models-in-your-candidate-model-set" class="level3">
<h3 class="anchored" data-anchor-id="fit-and-rank-models-in-your-candidate-model-set">Fit and rank models in your candidate model set</h3>
<p>Next, each model in the candidate model set is fit to your data and graded based on an estimate of the model’s “cost” vs.&nbsp;its “benefit”.</p>
<p>The model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters; see “The Principle of Parsimony” section below].</p>
<p>The model’s benefit is how well the model fits your data - i.e.&nbsp;how much of the variability in your response the model explains. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients (described in the <a href="./DSPPH_SM_StartingModel.html">Starting Model</a> section).</p>
<div class="callout callout-style-default callout-tip callout-titled" title="The Principle of Parsimony">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Principle of Parsimony
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The principle of parsimony means that, when in doubt, you will choose the simpler explanation. This means that:</p>
<ul>
<li><p>models should have as few parameters as possible</p></li>
<li><p>linear models are preferred to non-linear models</p></li>
<li><p>models with fewer assumptions are better</p></li>
</ul>
<p>This said, there are times when you might choose a more complicated explanation over a simpler explanation. One example of this is when you prioritize a model’s ability to predict a future response vs.&nbsp;getting an accurate understanding of the underlying mechanisms (e.g.&nbsp;using a model to accurately predict tomorrow’s weather vs.&nbsp;understanding the mechanisms behind tomorrow’s weather). We will discuss this more in the upcoming section on Prediction.</p>
</div>
</div>
</div>
<p>You can fit and rank your models quickly using a function called <code>dredge()</code> in the MuMIn package<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>. The <code>dredge()</code> function fits and ranks models representing all possible predictor combinations based on your validated starting model - i.e.&nbsp;your default candidate model set<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>. The output is a table ranking the models in your candidate model set.</p>
<p>Let’s explore this now.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MuMIn) <span class="co"># load MuMIn package</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">na.action =</span> <span class="st">"na.fail"</span>) <span class="co"># to avoid illegal model fitting</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>dredgeOut <span class="ot">&lt;-</span> <span class="fu">dredge</span>(validMod) <span class="co"># create model selection table for validated starting model</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(dredgeOut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Global model call: glm(formula = WtChange ~ Prey, family = gaussian(link = "identity"), 
    data = myDat)
---
Model selection table 
  (Intrc)    Prey df   logLik  AICc  delta weight
2  -6.996 0.07991  3  -29.864  66.1   0.00      1
1  -2.238          2 -125.489 255.2 189.07      0
Models ranked by AICc(x) </code></pre>
</div>
</div>
<p>Note the line:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">na.action =</span> <span class="st">"na.fail"</span>) <span class="co"># to avoid illegal model fitting</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is included because you need to make sure the data used to fit every model in your candidate model set stays the same. This could be violated if you have missing values in some of your predictor columns. This <code>options()</code> statement makes sure your model selection is following the rules.</p>
<p>The output of the <code>dredge()</code> function gives us</p>
<ul>
<li>the Global model call (our original hypothesis), and</li>
<li>a Model selection table</li>
</ul>
<p>The Model selection table contains one row for each model in our candidate model set. Let’s explore this now:</p>
<p><em>Find the column called “(Intrc)”</em>. This column tells us when the intercept is included in the model. If there is a number in that column, the associated model in your candidate model set (row) contains an intercept. Note that by default all models will contain an intercept<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>.</p>
<p><em>Find the column called “Prey”</em>. This column tells us when the <code>Prey</code> predictor is in the model. Notice the first row contains a number in the Prey column, while the second row is blank. This means that <code>Prey</code> is a predictor in the model reported in the first row but is missing from the model in the second. Note also that a number is recorded in the Prey column, row 1 (0.0799). This is the coefficient associated with the <code>Prey</code> predictor. Since we have a normal error distribution assumption, this coefficient can be considered the slope of a line<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. If the predictor was a categorical predictor (vs.&nbsp;numeric predictor), a “+” would appear in the Model selection table when the categorical predictor was in the model.</p>
<p>So, in our example above, the model in the first row contains an intercept and the <code>Prey</code> predictor - i.e.&nbsp;the first row is the model <code>WtChange ~ Prey + 1</code>. The model in the second row contains only an intercept - i.e.&nbsp;the second row is the model <code>WtChange ~ 1</code>.</p>
<p>The rest of the columns in the model selection table contain information that help us rank the models.</p>
<ul>
<li><p>the “df” column reports the number of model coefficients. Models that are more complicated (e.g.&nbsp;more predictors) will have a higher df as they require more coefficients to fit. Models with more terms are more “costly”. In the first row (<code>WtChange ~ Prey + 1</code>), df is 3 because the model fitting estimates three coefficients: one for the effect of <code>Prey</code> on <code>WtChange</code>, one for the intercept, and, since this is a normal error distribution assumption, one for the standard deviation. In the second row (<code>WtChange ~ 1</code>), df is 2 because the model fitting estimates a coefficient for the intercept, and for the normal error distribution assumption’s standard deviation. So the model in the first row is more “costly” than the second row.</p></li>
<li><p>the “logLik” column reports the log-Likelihood of the model fit. The absolute value of this estimate will depend on the type of data you are modelling, but in general, the logLik is related to how much variation in your response the model explains. It can be used to compare models fit to the same data. This can be seen as a measure of the “benefit” of the model.</p></li>
<li><p>the “AICc” column reports information criteria for your models. Information criteria balances the cost (complexity) and benefit (explained variation) for your model. An example of information criterion is the Akaike Information Criterion (AIC). The AIC is estimated as:</p></li>
</ul>
<p><span class="math inline">\(AIC = 2\cdot k - 2 \cdot ln(L)\)</span></p>
<p>where <span class="math inline">\(k\)</span> is the cost of the model (number of coefficients, like df above), and <span class="math inline">\(L\)</span> is the maximum likelihood estimate made when the model was fit (like logLik above).</p>
<p>There are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes). The AICc is reported by default here, but you can control that in the <code>dredge()</code> function. <strong>In all cases, lower information criterion means more support for the model.</strong></p>
<ul>
<li><p>the “delta” (<span class="math inline">\(\Delta\)</span>) column is a convenient way to see how different each model’s AICc is from the model with the lowest AICc (<span class="math inline">\(\Delta AIC_i\)</span> is the change in AIC for model <em>i</em> vs.&nbsp;the model with the lowest AIC.)</p></li>
<li><p>the “weight” column reports Akaike weights for the model. The Akaike weights are a measure of the relative likelihood of the models. The sum of all the Akaike weights is 1, so we can get a relative estimate for the support for each model.</p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Akaike weights">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Akaike weights
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here is the equation to estimate the Akaike weights:</p>
<p><span class="math display">\[
w_i = \frac{exp(-\frac{1}{2} \cdot \Delta AIC_i)}{\sum_{r=1}^{R}exp(-\frac{1}{2} \cdot \Delta AIC_r)}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(w_i\)</span> is the Akaike weight for model <em>i</em>,</li>
<li><span class="math inline">\(\Delta AIC_i\)</span> is the change in AIC for model <em>i</em> vs.&nbsp;the model with the lowest AIC</li>
<li><span class="math inline">\(\Delta AIC_r\)</span> is the change in AIC for model <em>r</em> vs.&nbsp;the model with the lowest AIC. This is estimated for all models in the candidate model set (<em>R</em> models).</li>
</ul>
<p><span class="citation" data-cites="Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference">Burnham and Anderson (<a href="#ref-Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference" role="doc-biblioref">2002</a>)</span></p>
</div>
</div>
</div>
</section>
<section id="choose-your-best-specified-models" class="level3">
<h3 class="anchored" data-anchor-id="choose-your-best-specified-models">Choose your best-specified model(s)</h3>
<p>Using the model selection table, you can choose your best-specified model(s) and find out what it tells you about your hypothesis.</p>
<p>In general, your best-specified model will be the model with the lowest information criterion (e.g.&nbsp;AIC)<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>. This will be the model at the top of the model selection table.</p>
<p>That said, notice I write “best-specified model(s)” - possibly plural. This is because you might have models where the AIC estimates are very close to one another. A good rule of thumb is to report all models where the AIC is within 2 of the lowest AIC model (i.e.&nbsp;delta &lt; 2). Following <span class="citation" data-cites="Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference">Burnham and Anderson (<a href="#ref-Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference" role="doc-biblioref">2002</a>)</span>,</p>
<table class="table">
<thead>
<tr class="header">
<th>for models where delta is</th>
<th>there is … for the model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0-2</td>
<td>substantial support</td>
</tr>
<tr class="even">
<td>4-7</td>
<td>considerably less support</td>
</tr>
<tr class="odd">
<td>&gt; 10</td>
<td>essentially no support</td>
</tr>
</tbody>
</table>
<p>Remember that ambivalence about which is the best model to explain variability in your response is a valid scientific result <span class="citation" data-cites="Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference">(<a href="#ref-Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference" role="doc-biblioref">Burnham and Anderson 2002</a>)</span>. Report all possible best-specified models and discuss what they mean for your research hypothesis and area, including possible follow-up studies that could be done to further the science in this area. More on this is discussed in the section on <a href="./DSPPH_SM_Communicating.html">Communicating</a>.</p>
<p>With our example above:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(dredgeOut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Global model call: glm(formula = WtChange ~ Prey, family = gaussian(link = "identity"), 
    data = myDat)
---
Model selection table 
  (Intrc)    Prey df   logLik  AICc  delta weight
2  -6.996 0.07991  3  -29.864  66.1   0.00      1
1  -2.238          2 -125.489 255.2 189.07      0
Models ranked by AICc(x) </code></pre>
</div>
</div>
<p>we have one best-specified model (model with substantial support):</p>
<p><code>WtChange ~ Prey + 1</code> (AICc = 66.1)</p>
<p>and essentially no support for the null model:</p>
<p><code>WtChange ~ 1</code> (AICc = 255.2; delta = 189.1)</p>
<p>We can conclude that there is evidence that <code>Prey</code> explains variability in <code>WtChange</code>.</p>
</section>
<section id="what-does-your-best-specified-models-say-about-your-hypothesis" class="level3">
<h3 class="anchored" data-anchor-id="what-does-your-best-specified-models-say-about-your-hypothesis">What does your best-specified model(s) say about your hypothesis?</h3>
<p>Model selection is a way of hypothesis testing. So what does your best-specified model say about your hypothesis? By comparing your best-specified model to your validated starting model, you can see where there is evidence for the effects of each predictor, and where the effects are estimated to be zero.</p>
<p>As our best-specified model is</p>
<p><code>WtChange ~ Prey + 1</code></p>
<p>We can conclude that there is evidence that <code>Prey</code> explains variability in <code>WtChange</code>.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="More examples">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
More examples
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>if our validated starting hypothesis was:</p>
<p><span class="math inline">\(Resp \sim Pred1 + Pred2 + Pred1:Pred2 + 1\)</span></p>
<ul>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim Pred1 + Pred2 + Pred1:Pred2 + 1\)</span> would indicate that we have evidence that there are effects of <code>Pred1</code> and <code>Pred2</code> on <code>Resp</code> and that the effect of <code>Pred1</code> on <code>Resp</code> depends on <code>Pred2</code> (an interaction).</p></li>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim Pred1 + Pred2 + 1\)</span> would indicate that we have evidence that there are effects of <code>Pred1</code> and <code>Pred2</code> on <code>Resp</code> but no evidence of an interaction effect.</p></li>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim Pred1 + 1\)</span> would indicate that we have evidence that there is an effect of <code>Pred1</code> but not <code>Pred2</code> on <code>Resp</code>.</p></li>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim 1\)</span> (i.e.&nbsp;the null hypothesis) would indicate that we have no evidence for effects of <code>Pred1</code> or <code>Pred2</code> on <code>Resp</code>. This is also a valid scientific result!</p></li>
</ul>
</div>
</div>
</div>
<p>In the next section (on <a href="DSPPH_SM_Reporting">Reporting</a>), we will discuss further how to communicate what your hypothesis testing results say about your hypothesis.</p>



</section>
</section>
</section>


<p>Copyright 2026, DSP Taskforce</p><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Burnham_Anderson_2002_Book_ModelSelectionMultimodelInference" class="csl-entry" role="listitem">
Burnham, K. P., and D. R. Anderson. 2002. <em>Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach</em>. Springer Verlag.
</div>
<div id="ref-Muff2022" class="csl-entry" role="listitem">
Muff, Stefanie, Erlend B. Nilsen, Robert B. O’Hara, and Chloé R. Nater. 2022. <span>“Rewriting Results Sections in the Language of Evidence.”</span> <em>Trends in Ecology &amp;Amp; Evolution</em> 37 (3): 203–10. <a href="https://doi.org/10.1016/j.tree.2021.10.009">https://doi.org/10.1016/j.tree.2021.10.009</a>.
</div>
<div id="ref-Popovic2024" class="csl-entry" role="listitem">
Popovic, Gordana, Tanya Jane Mason, Szymon Marian Drobniak, Tiago André Marques, Joanne Potts, Rocío Joo, Res Altwegg, et al. 2024. <span>“Four Principles for Improved Statistical Ecology.”</span> <em>Methods in Ecology and Evolution</em> 15 (2): 266–81. <a href="https://doi.org/10.1111/2041-210x.14270">https://doi.org/10.1111/2041-210x.14270</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>inference is the conclusion you make based on reasoning and evidence<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>vs.&nbsp;categorical<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>if you are confused why we are choosing these assumptions, read the <a href="./DSPPH_SM_StartingModel.html">Starting Model</a> section again<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>these estimates are called parameters<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>others not discussed here include cross-validation, Bayes Factors, confidence intervals<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>note that some write this as “<em>P</em> value” and some as “<em>p</em> value” and some as “<em>p</em>-value”. There is no one rule. Just pick one and make it consistent through your text. We will try to do that here.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>field<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>see more in the section on <a href="./handbookDCIntro.html">Data curation and collection</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>see more in the section on <a href="./DSPPH_SM_ModelValidation.html">Model Validation</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>because it is generally applicable<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>i.e.&nbsp;the effect of <code>Prey</code> on <code>WtChange</code><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>for example, a P value of 0.006 means that there is a 0.6% chance we would estimate the effect of <code>Prey</code> to be 0.12 <span class="math inline">\(g \cdot m^{3}\cdot num{-1}\)</span> when it was in fact 0<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>we’ll come back to this in the Reporting section<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>see more in the section on <a href="./DSPPH_SM_ModelValidation.html">Model Validation</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>these are also called “nested” models as each model is “nested” in one of the other models when it only differs by one predictor. “Nested” is also used in experimental design to mean something totally different, so we will avoid using the term here.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>note the spelling and capitalization of this package name!<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>other methods include forwards or backwards model selection. In these, you add (or subtract) one predictor term from your starting model to determine the effect on your model’s ability to explain the variability in your response. These methods can be biased by collinearity among your predictors in your model, so we will proceed with looking at all possible predictor combinations as described here<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>indeed, your null model <em>only</em> contains an intercept<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>and it is the same number given in the <code>summary()</code> output above. More on this coming up in the <a href="./DSPPH_SM_Reporting.html">Reporting section</a>!<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>Note that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright 2026, DSP Taskforce</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>