---
title: "Statistical Modelling: Reporting"
bibliography: handbook.bib

number-sections: true
number-depth: 2

---

```{r} 
#| echo: false
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```


```{r}
#| echo: false
library(png)
library(grid)
library(formatR)
```

::: {.callout-tip collapse="false"} 
## In this chapter you will learn:

- how to report your hypothesis testing results (including visualizations) to communicate what your model says about your hypothesis.

:::

# Reporting your hypothesis testing results

In an upcoming section, we will discuss [communicating your hypothesis testing (including results)](DSPPH_SM_Communicating.qmd) more formally - i.e. the text and visuals you will use to write your own scientific reports and papers.  In the current chapter, we will gather many of the pieces needed for that task.

Remember that testing your hypothesis means looking for evidence of an effect of each predictor on your response.  To report your model, you want to communicate this evidence, including:  

:::{.callout-note collapse="true" title="your best-specified model(s) "} 

You will report [your best-specified model](#sec_reportBestMod) by reporting the terms - the predictors and any interactions - that are in your model, and any from your starting model that were removed during model selection.

:::

:::{.callout-note collapse="true" title="how well your model explains your response "} 

You will report [how much variability in your response your best-specified model is able to explain](#sec_reportExplainedDeviance), and the relative importance of each model term - the predictors and any interactions - in explaining that variability.

:::

:::{.callout-note collapse="true" title="your modelled effects "} 

You will [report your modelled effects](#sec_reportEffects) - i.e. the effect of each predictor on your response.  Here you are interested in reporting:

- how big the effect of a predictor is on your response,

- how certain you are that there is an effect of the predictor on your response (is the effect bigger than zero?), and

- (for categorical predictors), if the effects differ among levels of a categorical predictor.

Remember that model effects are captured in your coefficients. 

:::


# Introducing the examples:

In this chapter, we will report model results for four example best-specified models that showcase a range of different hypotheses.  

If you are re-reading this chapter with your own hypothesis you are testing, look for the example that shares the most structure with your own hypothesis.  

The examples are:  

Example 1: `Resp1 ~ Cat1 + 1`

Example 2: `Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1`

Example 3: `Resp3 ~ Cont4 + 1`

Example 4: `Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1`

where 

* `Resp#` is your response variable. 
* `Cat#` indicates a categorical predictor
* `Cont#` indicates a continuous predictor


Note that each example is testing a different hypothesis by fitting a different model to a different data-set.  The four examples are not related to one another. 

Note also that the examples will all involve a normal error distribution assumption.  I will point out when your process would be different for a different error distribution assumption (e.g. Gamma, poisson, and binomial)

Finally, note that I am not going through all the steps that got us to these example best-specified models, but assume we went through the previous steps ([Responses](DSPPH_SM_Responses.qmd), [Predictors](DSPPH_SM_Predictors.qmd), [Hypothesis](DSPPH_SM_Hypothesis.qmd), [Starting model](DSPPH_SM_StartingModel.qmd), [Model validation](DSPPH_SM_ModelValidation.qmd) and [Hypothesis testing](DSPPH_SM_HypothesisTesting.qmd)) as outlined in this Handbook to choose each best-specified model.

:::{.callout-tip collapse="true" title="Example 1: Resp1 ~ Cat1 + 1"} 

#### Example 1: Resp1 ~ Cat1 + 1


```{r echo = FALSE}

rm(list=ls())
library(dplyr)

# Example 1: Resp1 ~ Cat1
 n=100
 ss<-sample(c(1:1000), 1)
 set.seed(679) #582
Cat1<-as.factor(sample(c("G", "K", "R"), size=n, replace=TRUE))

library(dplyr)
Group <- as.factor(sample(c("Site1", "Site2", "Site3", "Site4", "Site5", "Site6"),
                           n, replace=TRUE))
uResp<-(as.numeric(Cat1)*4.4+3.9*as.numeric(Group))#+sample(c(200:900), n, replace = TRUE)
Resp<-rnorm(n, uResp, 4.5)
Group <- recode(Group,
                    Site1 = 'Site3',
                    Site2 = 'Site1',
                    Site3 = 'Site5',
                   Site4 = 'Site4',
                   Site5 = 'Site2',
                   Site6 = 'Site6')
Group <- factor(Group, levels = c("Site1","Site2","Site3","Site4","Site5","Site6"))
Cat1 <- recode(Cat1,
                    K = 'Sp1',
                    R = 'Sp2',
                    G = 'Sp3')
Cat1<-factor(Cat1, levels = c("Sp1", "Sp2", "Sp3"))
myDat1<-data.frame(Cat1=Cat1, Resp1=round(Resp,1))
# #write.csv(myDat, "DatEx1.csv", row.names = FALSE)
# 
# library(ggplot2)
# ggplot()+
#   geom_point(data=myDat,
#              mapping=aes(x=Cat1, y=Resp1))
#
startMod<-glm(formula = Resp1 ~ Cat1 + 1, # hypothesis
              data = myDat1, # data
              family = gaussian(link="identity")) # error distribution assumption
# 
# 
# 
library(DHARMa)
# simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# #
# plot(simulationOutput, asFactor=TRUE) # compare simulated data to our observations
# #
# plot(simulationOutput,
#      form=myDat1$Cat1,
#      asFactor=TRUE) # compare simulated data to our observations
# # #
# myDat$Resid<-simulationOutput$scaledResiduals
# # # # 
# 
# # #
# ggplot()+
#   geom_violin(data=myDat,
#              mapping = aes(x=Group, y=Resid))
# 
# #
# #
library(MuMIn)
options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons
dredgeOut<-dredge(startMod) # fit and compare a model set representing all possible predictor combinations
#
bestMod<-(eval(attr(dredgeOut, "model.calls")$`2`)) # extract model #8 from dredge table
#
# #
# library(emmeans)
# forComp <- emmeans(bestMod, specs =  ~ Cat1, type = "response")
# forComp
# plot(forComp)
# plot(forComp, comparisons = TRUE)
# plot(pairs(emmeans(bestMod, "Cat1"),
#               adjust="scheffe"))

dredgeOut1<-dredgeOut
bestMod1<-bestMod

```

For example #1, assume your best-specified model says that your response variable (`Resp1`) is explained by a categorical predictor (`Cat1`):    

`Resp1 ~ Cat1`

Your best-specified model for example #1 is in an object called `bestMod1`:

```{r}

bestMod1

```
that was fit to data in `myDat1`:

```{r}

str(myDat1)

```

and the dredge() table you used to pick your bestMod1 is in `dredgeOut1`:

```{r}

dredgeOut1

```
:::

:::{.callout-tip collapse="true" title="Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1"} 


#### Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1


```{r echo = FALSE}

#rm(list=ls())
n=100
ss<-sample(c(1:1000), 1)
set.seed(114) #114
Cat2<-factor(sample(c("TypeA", "TypeB", "TypeC", "TypeD"), size=n, replace=TRUE))
Cat3<-factor(sample(c("Treat1", "Control"), size=n, replace=TRUE), levels=c("Treat1", "Treat2", "Control"))
uResp<-(as.numeric(Cat2)*40.4-33.3*as.numeric(Cat3)+ 23*as.numeric(Cat3)*as.numeric(Cat2))+sample(c(100:300), n, replace = TRUE)
Resp<-rnorm(n, uResp, 4.5)
Cat3[sample(which(Cat3=="Treat1"), round(length(which(Cat3=="Treat1"))/2), replace=TRUE)]<-"Treat2"
Cat2 <- recode(Cat2,
                    TypeC = 'TypeA',
                    TypeD = 'TypeB',
                    TypeB = 'TypeC',
                   TypeA = 'TypeD')
Cat2 <- factor(Cat2, levels = c("TypeA", "TypeB", "TypeC", "TypeD"))

myDat2<-data.frame(Resp2=Resp, Cat2=Cat2, Cat3=Cat3)
# # #write.csv(myDat2, "DatEx2.csv", row.names = FALSE)
# 
# ggplot()+
#   geom_point(data=myDat2,
#              mapping=aes(x=Cat2, y=Resp2, col=Cat3))
#
startMod<-glm(formula = Resp2 ~ Cat2 + Cat3 + Cat2:Cat3, # hypothesis
              data = myDat2, # data
              family = gaussian(link="identity")) # error distribution assumption



# library(DHARMa)
# simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# #
# plot(simulationOutput, asFactor=TRUE) # compare simulated data to our observations
# #
# plot(simulationOutput,
#      form=myDat2$Cat2,
#      asFactor=TRUE) # compare simulated data to our observations
# 
# plot(simulationOutput,
#      form=myDat2$Cat3,
#      asFactor=TRUE) # compare simulated data to our observations
# 
# # 
# #
# #
library(MuMIn)
options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons
dredgeOut<-dredge(startMod, extra = "R^2") # fit and compare a model set representing all possible predictor combinations
# dredgeOut
#
bestMod<-(eval(attr(dredgeOut, "model.calls")$`8`)) # extract model #8 from dredge table
#
# #
# library(emmeans)
# forComp <- emmeans(bestMod, specs =  ~ Cat1, type = "response")
# forComp
# plot(forComp)
# plot(forComp, comparisons = TRUE)
# plot(pairs(emmeans(bestMod, "Cat1"),
#               adjust="scheffe"))

dredgeOut2<-dredgeOut
bestMod2<-bestMod

```

For Example #2, assume your best-specified model says that your response variable (`Resp2`) is explained by two categorical predictors (`Cat2` & `Cat3`) as well as the interaction between the predictors:    

`Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1`

Your best-specified model for example #2 is in an object called `bestMod2`:

```{r}

bestMod2

```

that was fit to data in `myDat2`:

```{r}

str(myDat2)

```

and the `dredge()` table you used to pick your `bestMod2` is in `dredgeOut2`:

```{r}

dredgeOut2

```


:::

:::{.callout-tip collapse="true" title="Example 3: Resp3 ~ Cont4 + 1"} 

#### Example 3: Resp3 ~ Cont4 + 1


```{r echo = FALSE}

#rm(list=ls())
n=100
ss<-sample(c(1:1000), 1)
set.seed(261) #261
Cont4<-round(runif(n, 0.3, 20.9),2)
uResp<-245*Cont4+ 10
Resp<-rnorm(n, uResp, 850)
myDat3<-data.frame(Resp3=Resp, Cont4=Cont4)

# # #write.csv(myDat3, "DatEx3.csv", row.names = FALSE)


# ggplot()+
#   geom_point(data=myDat3,
#              mapping=aes(x=Cont4, y=Resp3))

startMod<-glm(formula = Resp3 ~ Cont4 + 1, # hypothesis
              data = myDat3, # data
              family = gaussian(link="identity")) # error distribution assumption


# # 
# library(DHARMa)
# simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# #
# plot(simulationOutput, asFactor=TRUE) # compare simulated data to our observations
# #
# plot(simulationOutput,
#      form=myDat3$Cont4,
#      asFactor=FALSE) # compare simulated data to our observations
# 
# #
# #
# #
library(MuMIn)
options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons
dredgeOut<-dredge(startMod) # fit and compare a model set representing all possible predictor combinations
#dredgeOut
#
bestMod<-(eval(attr(dredgeOut, "model.calls")$`2`)) # extract model #8 from dredge table
#
# #
# library(emmeans)
# forComp <- emmeans(bestMod, specs =  ~ Cat1, type = "response")
# forComp
# plot(forComp)
# plot(forComp, comparisons = TRUE)
# plot(pairs(emmeans(bestMod, "Cat1"),
#               adjust="scheffe"))

dredgeOut3<-dredgeOut
bestMod3<-bestMod

```

For Example #3, assume your best-specified model says that your response variable (`Resp3`) is explained by one continuous predictor (`Cont4`):    

`Resp3 ~ Cont4 + 1`

Your best-specified model for example #3 is in an object called `bestMod4`:

```{r}

bestMod3

```

that was fit to data in `myDat4`:

```{r}

str(myDat3)

```

and the `dredge()` table you used to pick your `bestMod3` is in `dredgeOut3`: 

```{r}

dredgeOut3

```
:::

:::{.callout-tip collapse="true" title="Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1"} 

#### Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1

```{r echo = FALSE}

#rm(list=ls())
n=100
ss<-sample(c(1:1000), 1)
set.seed(444) #444
Cat5<-as.factor(sample(c("Wild", "Farm", "Urban"), size=n, replace=TRUE))
Cont6<-round(runif(n, 300, 700),2)
uResp<-(as.numeric(Cat5)*0.014-0.02*Cont6+ as.numeric(Cat5)*0.014*Cont6)+100
Resp<-rnorm(n, uResp, 2.5)
myDat4<-data.frame(Resp4=Resp, Cat5=Cat5, Cont6=Cont6)

# # #write.csv(myDat3, "DatEx3.csv", row.names = FALSE)


# ggplot()+
#   geom_point(data=myDat3,
#              mapping=aes(x=Cont5, y=Resp3, col=Cat4))

startMod<-glm(formula = Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # hypothesis
              data = myDat4, # data
              family = gaussian(link="identity")) # error distribution assumption


# 
# library(DHARMa)
# simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# #
# plot(simulationOutput, asFactor=TRUE) # compare simulated data to our observations
# #
# plot(simulationOutput,
#      form=myDat3$Cat4,
#      asFactor=TRUE) # compare simulated data to our observations
# 
# plot(simulationOutput,
#      form=myDat3$Cont5,
#      asFactor=FALSE) # compare simulated data to our observations

#
#
# #
library(MuMIn)
options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons
dredgeOut<-dredge(startMod, extra = "R^2") # fit and compare a model set representing all possible predictor combinations
#dredgeOut
#
bestMod<-(eval(attr(dredgeOut, "model.calls")$`8`)) # extract model #8 from dredge table
#
# #
# library(emmeans)
# forComp <- emmeans(bestMod, specs =  ~ Cat1, type = "response")
# forComp
# plot(forComp)
# plot(forComp, comparisons = TRUE)
# plot(pairs(emmeans(bestMod, "Cat1"),
#               adjust="scheffe"))

dredgeOut4<-dredgeOut
bestMod4<-bestMod

```

For Example #4, assume your best-specified model says that your response variable (`Resp4`) is explained by one categorical predictor (`Cat4`) and one continuous predictor (`Cont6`) as well as the interaction between the predictors:    

`Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1`

Your best-specified model for example #3 is in an object called `bestMod4`:

```{r}

bestMod4

```

that was fit to data in `myDat4`:

```{r}

str(myDat4)

```

and the `dredge()` table you used to pick your `bestMod4` is in `dredgeOut4`:

```{r}

dredgeOut4

```
:::

# Reporting your best-specified models {#sec_reportBestMod}

::: {.alert .alert-warning}
This section is similar regardless of your model structure (e.g. error distribution assumption).  The examples below all assume a normal error distribution assumption but you can use the process presented here for models of any structure.
:::

Reporting your best-specified model means reporting the terms - the predictors and any interactions - that are in your model.  It is good practice to present the model along with the results from the model selection.  In this way, you can include multiple best-specified models if there is evidence that more than one might be useful.  Depending on your hypothesis and results, you may want to present all models within ∆AICc < 2 of the best-specified model, or all models with any Akaike weight, or simply all models.

Remember from the [Hypothesis Testing](DSPPH_SM_HypothesisTesting.qmd) chapter that you can also use the output from `dredge()` function to report evidence for how you picked the best-specified model.^[more to come in the section on [communicating](DSPPH_SM_Communicating.qmd)]


Let's take a look at how you do this with our examples:


:::{.callout-tip collapse="true" title="Example 1: Resp1 ~ Cat1 + 1"} 

#### Example 1: Resp1 ~ Cat1 + 1


```{r echo = FALSE}

dredgeOut1

```

For Example 1, you will report that your best-specified model is `Resp1 ~ Cat1 + 1`, i.e. that there is evidence that `Cat1` explains variability in `Resp1`.  This was chosen as the best-specified model as it had the lowest AICc and the next highest rank model had a ∆AICc of `r round(dredgeOut1$delta[2],2)` (i.e. ∆AICc > 2).  The best-specified model had an Akaike weight of `r round(dredgeOut1$weight[1],3)`.

:::

:::{.callout-tip collapse="true" title="Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1"} 

#### Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1

```{r}
#| echo: false


dredgeOut2

```

For Example 2, you will report that your best-specified model is `Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1`, i.e. that there is evidence that `Cat2` and `Cat3` explain variability in `Resp2`, and that there is an interaction between the effect of `Cat2` and `Cat3` on your response - i.e. the effect of `Cat2` on `Resp2` depends on the value of `Cat3`.  

This was chosen as the best-specified model as it had the lowest AICc and the next highest rank model had a ∆AICc of `r round(dredgeOut2$delta[2],2)` (i.e. ∆AICc > 2).  The best-specified model had an Akaike weight of `r round(dredgeOut2$weight[1],3)`.


:::

:::{.callout-tip collapse="true" title="Example 3: Resp3 ~ Cont4 + 1"} 

#### Example 3: Resp3 ~ Cont4 + 1

```{r}
#| echo: false


dredgeOut3

```
For Example 3, you will report that your best-specified model is `Resp3 ~ Cont4 + 1`, i.e. that there is evidence that `Cont4` explains variability in `Resp3`.  

This was chosen as the best-specified model as it had the lowest AICc and the next highest rank model had a ∆AICc of `r round(dredgeOut3$delta[2],2)` (i.e. ∆AICc > 2).  The best-specified model had an Akaike weight of `r round(dredgeOut3$weight[1],3)`.

:::


:::{.callout-tip collapse="true" title="Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1"} 

#### Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1

```{r}
#| echo: false


dredgeOut4

```
For Example 4, you will report that your best-specified model is `Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1`.  This model says that there is evidence that `Cat5` and `Cont6` explain variability in `Resp4`, and that there is an interaction between `Cat5` and `Cont6` - i.e. the effect of `Cont6` on `Resp4` depends on the value of `Cat5`.  

This was chosen as the best-specified model as it had the lowest AICc and the next highest rank model had a ∆AICc of `r round(dredgeOut4$delta[2],2)` (i.e. ∆AICc > 2).  The best-specified model had an Akaike weight of `r round(dredgeOut4$weight[1],3)`.

:::

# Reporting how well your model explains your response {#sec_reportExplainedDeviance}

::: {.alert .alert-warning}
This section is similar regardless of your model structure (e.g. error distribution assumption).  The examples below all assume a normal error distribution assumption but you can use the process presented here for models of any structure.
:::

In this section you will report

* how much variation (deviance) in your response is explained by your model

* how important each of your predictors is in explaining that deviance

## how much variation (deviance) in your response is explained by your model

If you will recall, your whole motivation for pursuing statistical modelling was to explain variation in your response.  Thus, it is important that you quantify how much variation in your response you are able to explain by your model.

Note that **we will discuss this as "explained deviance" rather than "explained variation"**.  This is because the term "variance" is associated with models where the error distribution assumption is normal, whereas deviance is a more universal term.  

When you have a **normal error distribution assumption and linear shape assumption**, you can capture the amount of explained deviance simply by comparing the variation in your response (i.e. the starting variation *before* you fit your model - the null variation) vs. the variation in your model residuals (i.e. the remaining variation *after* you fit your model - the residual variation) as the $R^2$:

$R^2 = 1 - \frac{residual variation}{null variation}$

From this equation, you can see how, if your model is able to explain all the variation in the response, the residual variation will be zero, and $R^2 = 1$.  Alternatively, if the model explains no variation in the response the residual variation equals the null variation and $R^2 = 0$.

For models with **other error distribution and shape assumptions**, you need another way of estimating the goodness of fit of your model.  You can do this through estimating a pseudo $R^2$. 

There are many many different types of pseudo $R^2$.^[check the [rr2 package](https://cran.r-project.org/web/packages/effectsize/vignettes/interpret.html) for many different options] One useful pseudo $R^2$ is called the Likelihood Ratio $R^2$ or $R^2_{LR}$.  The $R^2_{LR}$ compares the likelihood of your best-specified model to the likelihood of the null model:

$R^2_{LR} = 1-exp(-\frac{2}{n}(log𝓛(model)-log𝓛(null)))$

where $n$ is the number of observations, $log𝓛(model)$ is the log-likelihood of your model, and $log𝓛(null)$ is the log-likelihood of the null model.  The $R^2_{LR}$ is the type of pseudo $R^2$ that shows up in your `dredge()` output when you add `extra = "R^2"` to the `dredge()` call.  You can calculate $R^2_{LR}$ by hand, read it from our `dredge()` output, or estimate it using `r.squaredLR()` from the MuMIn package:

```{r}
r.squaredLR(bestMod1)
```

Note here that two values of $R^2_{LR}$ are reported. The adjusted pseudo $R^2$ given here under `attr(,"adj.r.squared")` has been scaled so that $R^2_{LR}$ can reach a maximum of 1, similar to a regular $R^2$^[this is called the Nagelkerke's modified statistic - see `?r.squaredLR` for more information].

Let's compare this to the traditional $R^2$: 

```{r}
1-summary(bestMod1)$deviance/summary(bestMod1)$null.deviance
```

Note that the two estimates are similar: One nice feature of the $R^2_{LR}$ is that it is equivalent to the regular $R^2$ when our model assumes a normal error distribution assumption and linear shape assumption, so you can use $R^2_{LR}$ for a wide range of models.


:::{.callout-note collapse="true" title="Aside: what is a *good* $R^2$?"} 

<img src="./goodR2.png" align="right" width="300px"/>

Recall that your goal in hypothesis testing is to explain the variation in your response - preferably as much variation as possible!  So it is natural to hope for a high `R^2` but this is a trap: the results of your hypothesis testing^[when done in a robust fashion] are valuable whether you are able to explain 3% or 93% of the variation in your response.  Remember:

- You are progressing science either way - helping the field learn about mechanisms and design future experiments.

- Remaining unexplained variation points to potential limitations in measuring predictor effects and/or other predictors (not in your model) that may play a role.

For context, here is a plot of `R^2` values from a meta-analysis of biology studies:

<br clear="right"/>




:::


## how important is each of your predictors in explaining that variation

When you have more than one predictor in your model, you may also want to report how relatively important each predictor is to explaining deviance in your response.  This is also called "partitioning the explained deviance" to the predictors or "variance decomposition".

:::{.callout-caution collapse="true" title="Aside: what we won't be doing"} 

To get an estimate of how much deviance in your response one particular predictor explains, you may be tempted to compute the explained deviance ($R^2$) estimates of models fit to data with and without that particular predictor.  Let's try with our Example 4:

```{r}

dredgeOut4

```

If you want to get an estimate as to how much response deviance the `Cont6` predictor explains, you might compare the $R^2$ of a model with and without the `Cont6` predictor.

Let's compare comparing model #4 (that includes `Cont6`) and model #2 (that doesn't include `Cont6`):

```{r}
R2.mod4 <- (dredgeOut4$`R^2`[2]) # model #4 is the second row in dredgeOut4
R2.mod2 <- (dredgeOut4$`R^2`[3]) # model #2 is the third row in dredgeOut4

diffR2 <- R2.mod4 - R2.mod2 # find estimated contribution of Cont to explained deviance

diffR2
```

So it looks like `r round(diffR2,3)*100`% of the variability in `Resp4` is explained by `Cont6`.

But what if instead you chose to compare model #3 (that includes `Cont6`) and model #1 (that doesn't include `Cont6`):

```{r}
R2.mod3 <- (dredgeOut4$`R^2`[4]) # model #3 is the fourth row in dredgeOut4
R2.mod1 <- (dredgeOut4$`R^2`[5]) # model #1 is the fifth row in dredgeOut4

diffR2 <- R2.mod3 - R2.mod1 # find estimated contribution of Cont to explained deviance

diffR2
```

Now it looks like `r round(diffR2,3)*100`% of the variability in `Resp4` is explained by `Cont6`! Quite a different answer!  Your estimates of the contribution of `Cont6` to explaining the response deviation don't agree because of collinearity among our predictors - more on this in the section on [Model Validation](DSPPH_SM_ModelValidation.qmd).

:::

There are a few options you can use to get a robust estimate of how much each predictor is contributing to explained deviance in your response.

One option for partitioning the explained deviance when you have collinearity among your predictors is hierarchical partitioning.  Hierarchical partitioning estimates the average independent contribution of each predictor to the total explained variance by considering all models in the candidate model set^[i.e. in the `dredge()` output].  This method is beyond the scope of the course but see the [rdacca.hp package](https://cran.r-project.org/web/packages/rdacca.hp/index.html) for an example of how to do this.  

Another method (**that we will be using**) for estimating the importance of each term (predictor or interaction) in your model is by again looking at the candidate model set ranking made by `dredge()`.  Here you can measure the importance of a predictor by summing up the Akaike weights for any model that includes a particular predictor.  The Akaike weight of a model compares the likelihood of the model scaled to the total likelihood of all the models in the candidate model set.  The sum of Akaike weights for models including a particular predictor tells you how important the predictor is in explaining the deviance in your response.  You can calculate the sum of Akaike weights directly with the `sw()` function in the MuMIn package:

```{r}

sw(dredgeOut4)

```

Here we can see that all model terms (the predictors `Cat5` and `Cont6` as well as the interaction) are equally important in explaining the deviance in `Resp4` (they appear in all models that have any Akaike weight).  

Let's look at these two steps 

* how much deviance in your response is explained by your model

* how important each of your predictors is in explaining that deviance

with our examples:

:::{.callout-tip collapse="true" title="Example 1: Resp1 ~ Cat1 + 1"} 

#### Example 1: Resp1 ~ Cat1 + 1

* how much deviance in your response is explained by your model

```{r}
R2 <- r.squaredLR(bestMod1)

R2

```

The best-specified model explains `r round(data.frame(R2),3)*100`% of the deviance in `Resp1`.

* how important each of your predictors is in explaining that deviance


```{r}

dredgeOut2

```

```{r}

sw(dredgeOut1)

```


With Example 1, you have only one predictor (`Cat1`) and so this predictor is responsible for explaining all of the variability in your response (`Resp1`).  You can see that it appears in all models with any weight with your `sw()` function from the MuMIn package.



:::



:::{.callout-tip collapse="true" title="Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1"} 

#### Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1

* how much deviance in your response is explained by your model

```{r}
R2 <- r.squaredLR(bestMod2)

R2

```

The best-specified model explains `r round(data.frame(R2),3)*100`% of the deviance in `Resp2`.

* how important each of your predictors is in explaining that deviance

```{r}

dredgeOut2

```


```{r}

sw(dredgeOut2)

```

Here you can see that `Cat2` and `Cat3` are equally important in explaining the deviance in `Resp2` (they appear in all models that have any Akaike weight), while the interaction term between `Cat2` and `Cat3` is less important (only appearing in one model with Akaike weight, though this is the top model).  


:::

:::{.callout-tip collapse="true" title="Example 3: Resp3 ~ Cont4 + 1"} 

#### Example 3: Resp3 ~ Cont4 + 1

* how much deviance in your response is explained by your model

```{r}
R2 <- r.squaredLR(bestMod3)

R2

```

The best-specified model explains `r round(data.frame(R2),3)*100`% of the deviance in `Resp3`.

* how important each of your predictors is in explaining that deviance

```{r}

dredgeOut3

```


```{r}

sw(dredgeOut3)

```

With Example 3, you have only one predictor (`Cont6`) and so this predictor is responsible for explaining all of the variability in your response (`Resp3`).  You can see that it appears in all models with any weight with your `sw()` function from the MuMIn package.
 

:::

:::{.callout-tip collapse="true" title="Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1"} 

#### Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1


* how much deviance in your response is explained by your model

```{r}
R2 <- r.squaredLR(bestMod4)

R2

```

The best-specified model explains `r round(data.frame(R2),3)*100`% of the deviance in `Resp2`.

* how important each of your predictors is in explaining that deviance

```{r}

dredgeOut4

```


```{r}

sw(dredgeOut4)

```

Here you can see that `Cat5`, `Cont6` and the interaction `Cat5:Cont6` are all equally important in explaining the deviance in `Resp4` (they appear in all models that have any Akaike weight).


:::


# Reporting your modelled effects {#sec_reportEffects}

In the last chapter of these notes, we discussed [testing your hypothesis](DSPPH_SM_HypothesisTesting.qmd).  

Remember that testing your hypothesis is looking for evidence of **effects** of your predictors on your response.  By testing your hypothesis, you determined that there was evidence that the effects of your predictors on your response are non-zero [@Popovic2024]. 

These effects are presented as estimates of the coefficients in your model (e.g. a slope) which are called parameters once they have been estimated.

For *categorical* predictors, the coefficient represents how much your response changes (increases or decreases) when you change from one level of the category to another.  For *numeric* predictors, the coefficient represents how much your response changes (increases or decreases) when you increase your predictor by one unit.  More on this below!

Here you will gather the information to communicate visually and quantitatively the magnitude and direction of your effects and determine (in some cases) where effects are similar or different from one another, answering questions like:

- how much does a change (effect) in your predictor change your response?

- is that change (effect) positive (your response) or negative (your response decreases)?

- is that change (effect) similar across levels of your categorical predictor?

You can report your modelled effects in a number of ways^[with some options being more or less helpful depending on your model structure]:

1) **Visualizing your model effects**: Here, you will make plots of your model effects including the effects themselves, uncertainty around the effects and your observations.  These plots will help communicate your modelled effects as well as how well your model fits your observations.

2) **Giving examples of your modelled effects**: Here, you will use your model to predict the value of your response (with uncertainty) to illustrate how the response changes when your predictors change.  

3) **Quantifying your model effects**: Here, you will report (in numbers) the magnitude and direction of your modelled effects along with the uncertainty. Exactly how you do that will depend on the structure of your model (e.g. the error distribution assumption).  We will go through a number of examples here. You will also look for evidence of whether modelled effects of a categorical predictor differ across all levels.  

We will focus on options #1 and 3 here as giving examples of your modelled effects is covered^[also see the "by hand" visualization method below] in the section on [predicting](DSPPH_SM_Predicting.qmd).

## Visualizing your model effects

::: {.alert .alert-warning}
This section is similar regardless of your model structure (e.g. error distribution assumption).  The examples below all assume a normal error distribution assumption but you can use the process presented here for models of any structure.
:::

Here, you will visualize how each predictor is affecting the response by drawing how your modelled fitted values (the estimates of your response made by the model) change with your predictors, along with a measure of uncertainty around those fitted values.  You also want to include your data (actual observations of your response and predictors) so you can start to communicate how well your model captures your hypothesis, and what amount of deviance in your response is explained by your model.  

There are a lot of different R packages that make it easy to quickly visualize your model.  We will focus on two methods that will allow you to make quick visualizations of your model and/or customize the figures as you would like.

* *visualizing using the visreg package*: This will let you get a quick look at your model object with a limited amount of customization. 

* *vizualizing "by hand"*: Here, "by hand" is a bit of a silly description as R will be doing the work for you.  What I mean by "by hand" is that you will be building the plot yourself by querying your model object. This method is very flexible and will lead you to a fully customizable visualization.  This process involves i) choosing the values of your predictors at which to make estimates of your fitted values, ii) using `predict()` to use your model to estimate your response variable at those values of your predictors, and iii) use the model estimates to plot your model fit.  Aside: this method is also what is used to make predictions from your model - more in the [Predicting](DSPPH_SM_Predicting.qmd) section. 

Below are visualizations for each of our examples.  Two further tips:

- In a few of the examples, we use ggplot2's `position_dodge` argument.  Be careful here - it can be useful for avoiding overlapping data points when predictors are categorical BUT can actually change the location of data points if used with a numeric predictor.

- Note that we *do not* use the `geom_smooth()` function in the ggplot2 package.  The fits provided by `geom_smooth()` are not the same as the model fits directly from your best-specified model so we avoid this here (e.g. `geom_smooth()` will fit a separate model for each level of your categorical predictor negating the hypothesis testing that allows us to make comparisons across the predictor levels).


:::{.callout-tip collapse="true" title="Example 1: Resp1 ~ Cat1 + 1"} 

###### Example 1: Resp1 ~ Cat1 + 1

**Visualizing with the visreg package** 


```{r}

library (visreg) # load visreg package for visualization

library(ggplot2) # load ggplot2 package for visualization

visreg(bestMod1, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cat1", # predictor on x-axis
       #by = ..., # if you want to include a 2nd predictor plotted as colour
       #breaks = ..., # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 3rd predictor
       #overlay = TRUE, # to plot as overlay or panels, when there is 
       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals
       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cat1, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme


```

Note the small dashes at the top and bottom of the plot.  This is called the "rug".  visreg uses the rug to give an indication of where your observed data lie - a dash on the bottom axis means you have a data point at that location that would be a positive residual relative to the model fit; a dash on the top axis means you have a data point at that location that would be a positive residual relative to the model fit.  This is useful, but not the same as actually including your observations on the plot.  Unfortunately due to limitations of the visreg package, you can not easily add you observations onto plots where the x-axis is a categorical predictor.  But that's ok, because there are other options...

**Visualizing by hand**

To plot by hand, you will 

i) first make a data frame containing the value of your predictors at which you want to plot effects on the response: 


```{r}

# Set up your predictors for the visualized fit
forCat1<-unique(myDat1$Cat1) # every value of your categorical predictor

# create a data frame with your predictors
forVis<-expand.grid(Cat1=forCat1) # expand.grid() function makes sure you have all combinations of predictors

```


ii) Next, you will use the `predict()` function^[you will be using the `predict()` function again in the [Predicting](DSPPH_SM_Predicting) chapter] to get the model estimates of your response variable at those values of your predictors:

```{r}

# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod1, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod1)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame


```



iii) Finally, you will use these model estimates to make your plot:

```{r}

library(ggplot2)

ggplot() + # start ggplot
  geom_point(data = myDat1, # add observations to your plot
             mapping = aes(x = Cat1, y = Resp1), 
             position=position_jitter(width=0.1)) + # control position of data points so they are easier to see on the plot
  geom_errorbar(data = forVis, # add the uncertainty to your plot
              mapping = aes(x = Cat1, y = Fit, ymin = Lower, ymax = Upper),
              linewidth=1.2) + # control thickness of errorbar line
  geom_point(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cat1, y = Fit), 
             shape = 22, # shape of point
             size = 3, # size of point
             fill = "white", # fill colour for plot
             col = 'black') + # outline colour for plot
  ylab("Resp1, (units)")+ # change y-axis label
  xlab("Cat1, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

```

:::




:::{.callout-tip collapse="true" title="Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1"} 

###### Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1

**Visualizing with the visreg package** 

As you have more than one predictor in Example 2, there are a number of ways you can visualize your effects.  

Here is an example of a visualization with `Cat2` on the x-axis and the effects of `Cat3` plotted as separate panels:

```{r}

library (visreg) # load visreg package for visualization

library(ggplot2) # load ggplot2 package for visualization

visreg(bestMod2, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cat2", # predictor on x-axis
       by = "Cat3", # if you want to include a 2nd predictor plotted as colour
       #breaks = ..., # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 3rd predictor
       overlay = FALSE, # to plot as overlay or panels, when there is 
       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals
       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cat2, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

# tip: we can't include our observations on this plot due to the limitations of the visreg package when plotting with a categorical predictor on the x-axis

```


And here is an example of a visualization with `Cat2` on the x-axis and the effects of `Cat3` overlayed on the plot as different colours for each category (level) of `Cat3`:

```{r}

visreg(bestMod2, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cat2", # predictor on x-axis
       by = "Cat3", # if you want to include a 2nd predictor plotted as colour
       #breaks = ..., # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 3rd predictor
       overlay = TRUE, # to plot as overlay or panels, when there is 
       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals
       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cat2, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

# tip: we can't include our observations on this plot due to the limitations of the visreg package when plotting with a categorical predictor on the x-axis

```

Note the small dashes at the top and bottom of the plot.  This is called the "rug".  visreg uses the rug to give an indication of where your observed data lie - a dash on the bottom axis means you have a data point at that location that would be a positive residual relative to the model fit; a dash on the top axis means you have a data point at that location that would be a positive residual relative to the model fit.  This is useful, but not the same as actually including your observations on the plot. Unfortunately due to limitations of the visreg package, you can not easily add you observations onto plots where the x-axis is a categorical predictor.  But that's ok, because there are other options...



**Visualizing by hand**

```{r}

#### i) choosing the values of your predictors at which to make a prediction

# Set up your predictors for the visualized fit
forCat2<-unique(myDat2$Cat2) # every level of your categorical predictor
forCat3<-unique(myDat2$Cat3) # every level of your categorical predictor
  
# create a data frame with your predictors
forVis<-expand.grid(Cat2 = forCat2, Cat3 = forCat3) # expand.grid() function makes sure you have all combinations of predictors

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod2, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod2)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame

#### iii) use the model estimates to plot your model fit

library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  geom_point(data = myDat2, # add observations to your plot
             mapping = aes(x = Cat2, y = Resp2, col = Cat3), 
             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot
  geom_errorbar(data = forVis, # add the uncertainty to your plot
              mapping = aes(x = Cat2, y = Fit, ymin = Lower, ymax = Upper, col = Cat3),
              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot
              size=1.2) + # control thickness of errorbar line
  geom_point(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cat2, y = Fit, fill = Cat3), 
             shape = 22, # shape of point
             size=3, # size of point
             col = 'black', # outline colour for point
             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot
  ylab("Resp2, (units)")+ # change y-axis label
  xlab("Cat2, (units)")+ # change x-axis label
  labs(fill="Cat3, (units)", col="Cat3, (units)") + # change legend title
  theme_bw() # change ggplot theme



```

:::



:::{.callout-tip collapse="true" title="Example 3: Resp3 ~ Cont4 + 1"} 

###### Example 3: Resp3 ~ Cont4 + 1


**Visualizing with the visreg package** 

Notice how you can include the `gg = TRUE` argument to plot this as a ggplot type figure.  This allows you to add your data onto the visualization of your model.

```{r}

library (visreg) # load visreg package for visualization

library(ggplot2) # load ggplot2 package for visualization

visreg(bestMod3, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cont4", # predictor on x-axis
       #by = ..., # if you want to include a 2nd predictor plotted as colour
       #breaks = ..., # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 3rd predictor
       #overlay = TRUE, # to plot as overlay or panels, when there is 
       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals
       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.
  geom_point(data = myDat3,
             mapping = aes(x = Cont4, y = Resp3))+
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cont4, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

# Note: you CAN include your observations on your plot with visreg when you have a continuous predictor on the x-axis

```

Note the small dashes at the top and bottom of the plot.  This is called the "rug".  visreg uses the rug to give an indication of where your observed data lie - a dash on the bottom axis means you have a data point at that location that would be a positive residual relative to the model fit; a dash on the top axis means you have a data point at that location that would be a positive residual relative to the model fit.  

**Visualizing by hand**
```{r}

#### i) choosing the values of your predictors at which to make a prediction


# Set up your predictors for the visualized fit
forCont4<-seq(from = min(myDat3$Cont4), to = max(myDat3$Cont4), by = 1)# a sequence of numbers in your continuous predictor range
  
# create a data frame with your predictors
forVis<-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod3, # the model
                newdata = forVis, # the predictor values
                type = "link",# here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate


ilink <- family(bestMod3)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame

#### iii) use the model estimates to plot your model fit


library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  
  geom_point(data = myDat3, # add observations to your plot
             mapping = aes(x = Cont4, y = Resp3)) + # control position of data points so they are easier to see on the plot
  
  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont4, y = Fit),
              linewidth = 1.2) + # control thickness of line
  
    geom_line(data = forVis, # add uncertainty to your plot (upper line)
              mapping = aes(x = Cont4, y = Upper),
              linewidth = 0.8, # control thickness of line
              linetype = 2) + # control style of line
  
      geom_line(data = forVis, # add uncertainty to your plot (lower line)
              mapping = aes(x = Cont4, y = Lower),
              linewidth = 0.8, # control thickness of line
              linetype = 2) + # control style of line
  
  ylab("Resp3, (units)") + # change y-axis label
  
  xlab("Cont4, (units)") + # change x-axis label
  
  theme_bw() # change ggplot theme


# another option making use of geom_ribbon()

ggplot() + # start ggplot
  
  geom_point(data = myDat3, # add observations to your plot
             mapping = aes(x = Cont4, y = Resp3)) + # control position of data points so they are easier to see on the plot
  
  geom_ribbon(data = forVis, # add uncertainty to your plot (upper line)
          mapping = aes(x = Cont4, ymin = Lower, ymax = Upper),
          alpha = 0.3) + # control the transparency, between 0 (transparent) and 1 (opaque)

  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont4, y = Fit),
              linewidth = 1.2) + # control thickness of line
  
  ylab("Resp3, (units)") + # change y-axis label
  
  xlab("Cont4, (units)") + # change x-axis label
  
  theme_bw() # change ggplot theme



```

:::


:::{.callout-tip collapse="true" title="Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1"} 

###### Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1

plotting with marginal effects can be 4 D with x-axis, colour, row and columns
**Visualizing with the visreg package** 

As you will see in @sec_quantifying, there is no evidence of an effect of `Cont6` on `Resp3` when `Cat5 = Farm` - i.e. the coefficient (slope) is not different from zero.  In such cases, you would typically **not** include the effect on the plot (red line in the next figure), but I include it here to illustrate the visualization techniques.

A plot with `Cont6` on the x-axis:

```{r}

library (visreg) # load visreg package for visualization

library(ggplot2) # load ggplot2 package for visualization

visreg(bestMod4, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cont6", # predictor on x-axis
       by = "Cat5", # predictor plotted as colour
       #breaks = 3, # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 4th predictor
       overlay = TRUE, # to plot as overlay or panels 
       rug = FALSE, # to include a rug
       gg = TRUE)+ # to plot as a ggplot
  geom_point(data = myDat4, # data
             mapping = aes(x = Cont6, y = Resp4, col = Cat5))+ # add data to your plot
  #ylim(0, 60)+ # adjust the y-axis units
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cont6, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

```

A plot with `Cat5` on the x-axis: Notice how the continuous predictor is represented by different levels in the colours on the plot.  Here you've asked for three levels with `breaks = 3`.  Note that you can not include your observations on the visreg plot when the x-axis predictor is a category

```{r}


visreg(bestMod4, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cat5", # predictor on x-axis
       by = "Cont6", # predictor plotted as colour
       breaks = 3, # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 4th predictor
       overlay = TRUE, # to plot as overlay or panels 
       rug = FALSE, # to include a rug
       gg = TRUE)+ # to plot as a ggplot
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cat5, (units)")+ # change x-axis label
  labs(fill="Cont6, (units)", col="Cont6, (units)") + # change legend title
  theme_bw() # change ggplot theme

```

You can also specify at which levels the breaks should occur with the `breaks = ...` argument.  For example, you can ask visreg to plot the modelled effects when `Cont6` = 400 and `Cont6` = 600:

```{r}

visreg(bestMod4, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cat5", # predictor on x-axis
       by = "Cont6", # predictor plotted as colour
       breaks = c(400,600), # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 4th predictor
       overlay = TRUE, # to plot as overlay or panels 
       rug = FALSE, # to include a rug
       gg = TRUE)+ # to plot as a ggplot
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cat5, (units)")+ # change x-axis label
  labs(fill="Cont6, (units)", col="Cont6, (units)") + # change legend title
  theme_bw() # change ggplot theme


```

Note that you can not include your observations on the visreg plot when the x-axis predictor is a category.



**Visualizing by hand**

Here's an example with `Cont6` on the x-axis:

```{r}

#### i) choosing the values of your predictors at which to make a prediction

# Set up your predictors for the visualized fit
forCat5<-unique(myDat4$Cat5) # all levels of your categorical predictor
forCont6<-seq(from = min(myDat4$Cont6), to = max(myDat4$Cont6), by = 1)# a sequence of numbers in your continuous predictor range
  
# create a data frame with your predictors
forVis<-expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod4, # the model
                newdata = forVis, # the predictor values
                type = "link",# here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod4)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame

#### iii) use the model estimates to plot your model fit


library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  
  geom_point(data = myDat4, # add observations to your plot
             mapping = aes(x = Cont6, y = Resp4, col = Cat5)) + # control position of data points so they are easier to see on the plot
  
  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont6, y = Fit, col = Cat5),
              linewidth = 1.2) + # control thickness of line
  
    geom_line(data = forVis, # add uncertainty to your plot (upper line)
              mapping = aes(x = Cont6, y = Upper, col = Cat5),
              linewidth = 0.4, # control thickness of line
              linetype = 2) + # control style of line
  
      geom_line(data = forVis, # add uncertainty to your plot (lower line)
              mapping = aes(x = Cont6, y = Lower, col = Cat5),
              linewidth = 0.4, # control thickness of line
              linetype = 2) + # control style of line
  
  ylab("Resp4, (units)") + # change y-axis label
  
  xlab("Cont6, (units)") + # change x-axis label
  
  labs(fill="Cat5, (units)", col="Cat5, (units)") + # change legend title
  
  theme_bw() # change ggplot theme



# another option making use of geom_ribbon()

ggplot() + # start ggplot
  
  geom_point(data = myDat4, # add observations to your plot
             mapping = aes(x = Cont6, y = Resp4, col = Cat5)) + # control position of data points so they are easier to see on the plot
  
  geom_ribbon(data = forVis, # add uncertainty to your plot (upper line)
          mapping = aes(x = Cont6,  ymin = Lower, ymax = Upper, fill = Cat5),
          alpha = 0.3) + # control the transparency, between 0 (transparent) and 1 (opaque)

  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont6, y = Fit, col = Cat5),
              linewidth = 1.2) + # control thickness of line
  
  ylab("Resp4, (units)") + # change y-axis label
  
  xlab("Cont6, (units)") + # change x-axis label
  
  labs(fill="Cat5, (units)", col="Cat5, (units)") + # change legend title
  
  theme_bw() # change ggplot theme




```



Here's an example with `Cat5` on the x-axis:

```{r}

#### i) choosing the values of your predictors at which to make a prediction

# Set up your predictors for the visualized fit
forCat5<-unique(myDat4$Cat5) # all levels of your categorical predictor
forCont6<-c(400, 600) # a sequence of numbers in your continuous predictor range
  
# create a data frame with your predictors
forVis<-expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor

# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod4, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod4)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame


#### iii) use the model estimates to plot your model fit


library(ggplot2) # load ggplot2 library


ggplot() + # start ggplot
  
  geom_point(data = myDat4, # add observations to your plot
             mapping = aes(x = Cat5, y = Resp4, col = Cont6), 
             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot
  
  geom_errorbar(data = forVis, # add the uncertainty to your plot
              mapping = aes(x = Cat5, y = Fit, ymin = Lower, ymax = Upper, col = Cont6),
              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot
              size=1.2) + # control thickness of errorbar line
  
  geom_point(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cat5, y = Fit, fill = Cont6), 
             shape = 22, # shape of point
             size=3, # size of point
             col = 'black', # outline colour for point
             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot
  
  ylab("Resp4, (units)")+ # change y-axis label
  
  xlab("Cat5, (units)")+ # change x-axis label
  
  labs(fill="Cont6, (units)", col="Cont6, (units)") + # change legend title
  
  theme_bw() # change ggplot theme

```


:::{.callout-note collapse="true" title="Aside: Plotting in 3D"} 

You might notice that many of the plots above are communicating 3-dimensions (one response + two predictors) in a 2-dimensional plot.  There are other ways of making 3-dimensional plots in R, e.g. with the visreg package using the `visreg2d()` function in the visreg package:


```{r}

visreg2d(fit = bestMod4, # your model
         xvar = "Cont6", # what to plot on the x-axis
         yvar = "Cat5", # what to plot on the y-axis
         scale = "response") # make sure fitted values (colours) are on the scale of the response

```

or "by hand" using the `geom_raster()` function in the ggplot2 package:

```{r}

# Set up your predictors for the visualized fit
forCont6<-seq(from = min(myDat4$Cont6), 
             to = max(myDat4$Cont6), 
             by = 0.1) # e.g. a sequence of Cont values
forCat5<-unique(myDat4$Cat5) # every value of your categorical predictor

# create a data frame with your predictors
forVis<-expand.grid(Cont6=forCont6, Cat5=forCat5) # expand.grid() function makes sure you have all combinations of predictors


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod4, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod4)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame

# create your plot
ggplot() + # start your ggplot
  geom_raster(data = forVis, aes(x = Cont6, y = Cat5, fill = Fit))+ # add the 3 dimensions as a raster
  geom_point(data = myDat4, aes(x = Cont6, y = Cat5, colour = Resp)) # add your data
```

**EXTRA**: As you can see, these plots represent the 3rd dimension by using colour.  You can also make actual 3 dimensional plots in R with the [plotly package](https://plotly.com/r/getting-started/).  These plots are interactive which makes them more useful than static 3d plots.  **Click on the plot and move your mouse to rotate the plot!**


```{r}

library(plotly) # load the plotly package

plot_ly(data = forVis, # the data with your model predictions (made above)
        x = ~Cont6, # what to plot on the x axis
        y = ~Cat5, # what to plot on the y axis
        z = ~Fit, # what to plot on the z axis
        type="scatter3d", # type of plot
        mode="markers") %>% # type of plot
  add_markers(data = myDat4, x = ~Cont6, y = ~Cat5, z = ~Resp4) # add your data

```


:::

:::


## Quantifying your model effects {#sec_quantifying}

::: {.alert .alert-danger}
How you quantify your model effects varies with your model structure.  If it is your first time reading this, read through the examples that present models assuming a normal error distribution assumption.  This will help you understand why we are reporting modelled effects in this way.  Then, (if relevant) look at the section on the error distribution assumption you are interested in for your model. 
:::

In this section, you will learn how to report your modelled effects in numbers. In general, this section will involve answering:

* What are your modelled effects (the magnitude, direction and uncertainty of your coefficient estimates)?

If you also have a categorical predictor with more than two levels, you will also want to answer:

* Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)

Let's examine each of these in turn:

### What are your modelled effects (with uncertainty)?

Recall from the text above that your model effects describe the change in your response given a change in your predictor.  These effects are given in the estimates of your coefficients.

When you have a numeric predictor, the coefficient gives the amount of change in the response that is caused by a unit change in your numeric predictor.  When the coefficient is positive, you have an increase in your response with a unit change in your predictor.  When the coefficient is negative, you have a decrease in your response with a unit change in your predictor. When the coefficient is zero, you have no change in your response for a unit change in your predictor, and you would conclude that there is no evidence that your predictor explains variability in your response. 

For linear models, information about the effect of a numeric predictor is captured in the coefficient (slope) of the linear fit, (Expand the text box below to see why), and this effect is the same across the entire range of your predictor (the definition of a linear relationship).  When you have a numeric predictor in your best-specified model, you can conclude that there is evidence that it is explaining variation in your response.

:::{.callout-note collapse="true" title="the math"} 

Here's a model equation for a generalized linear model:

$$
\begin{align}
g(\mu_i|Pred_i) &= \beta_1 \cdot Pred_i + \beta_0 \\
Resp_i &\sim F(\mu_i) 
\end{align}
$$

where 
* $g()$ is the link function
* $\mu_i$ is the mean fitted value $\mu_i$ dependent on $Pred_i$
* $\beta_1$ is the coefficient of $Pred_i$
* $Pred_i$ is the value of your predictor
* $\beta_0$ is the intercept
* $Resp_i$ is the value of the response variable
* $F$ represents some error distribution assumption



As above, the effect of a continuous predictor on your response is the change in your fitted value ($g(\mu_i)$) for a unit change in your predictor ($Pred_i$):

$$
\begin{align}
g(\mu|Pred+1) - g(\mu|Pred) &= (\beta_1 \cdot (Pred+1) + \beta_0) - (\beta_1 \cdot Pred_i + \beta_0) \\
&=\beta_1 \cdot Pred  + \beta_1 + \beta_0 - \beta_1 \cdot Pred_i - \beta_0 \\
&=\beta_1
\end{align}
$$
For linear models with a normal error distribution assumption (i.e. when you are using `link = "identity"`), the link function $g()$ is not there.  For this reason, you can read the effects of your model directly from the model coefficients ($\beta_1$, $\beta_0$) when you have a normal error distribution assumption.  And for this reason, you will need to convert your model coefficients to quantify your modelled effects when you have an error distribution assumption that is not normal/uses a link function that isn't "identity".  


<img src="./linkVsResponse.png" align="right" width="300px"/>

This is because of the difference between the "link" scale and "response" scale for generalized linear models.

The link scale is where the model fitting happens, and where the evidence for your modelled effects is gathered.

The response scale is back in the real world - these are the actual effects in your response you would expect to see with a change in your predictor.
<br clear="right"/>

Recall that you can see the default link functions associated with each error distribution function with `?family`:

<img src="./familyLinks.png" width="300px"/>

:::

When you have a categorical predictor, the coefficient is the intercept of the linear fit and the effect of the predictor on your response is captured in how that intercept changes as you move from one level of your category to another.  So if you have a categorical predictor with 3 categories (levels), you will have three coefficients: one coefficient that describes the mean response level for one category, and two coefficients that describe how this mean changes when you change categories. A positive coefficient for level B means that there is a higher mean response value when your predictor is level B vs. level A. A negative coefficient for level B means that there is a lower mean response value when your predictor is level B vs. level A.  A coefficient for level B that is zero means that there is no evidence of a difference in your response value when your predictor is level B vs. level A. 

Finally, you need to report the uncertainty around your modelled effects so your peers know how much evidence there is for these modelled effects.  There are a number of ways to do this.  Two common ones are i) reporting standard errors (SE) which are a measure of uncertainty of the average modelled effect, and ii) the 95% confidence intervals around your modelled effects, which tell you the range your modelled effects would be expected to fall into (with a 95% probability) if you redid your experiment. We will focus on the 95% confidence intervals as they tend to be more generally interperable across different types of models.

The meaning behind your coefficients outlined above are true regardless of your model structure BUT our interpretation of what the effects mean in the real world will vary based on e.g. your error distribution assumptions (see the text box above for an explanation).  In the examples below, you will learn how to quantify your modelled effects and uncertainty estimates for models with a normal error distribution assumption.  After this, there are sections below to show you how to do the same for models with other error distribution assumptions.

For a quick summary, here is how the method depends on your modelled error distribution assumption:

<img src="./convertCoef.png" width="700px"/>


  

### Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)

When you have a categorical predictor in your best-specified model, you know there is evidence of an effect for at least one of your levels (categories).  You will also need to determine the evidence for the the effects among your factor levels, where they differ from zero and where they differ from each other. The processes for this outlined below can be applied to models of any structure. 

### Models with a normal error distribution assumption: {#sec-normalModelEffects}


:::{.callout-tip collapse="true" title="Example 1: Resp1 ~ Cat1 + 1"} 

###### Example 1: Resp1 ~ Cat1 + 1


Recall that Example 1 contains only one predictor and it is categorical:

`Resp1 ~ Cat1 + 1`


**What are your modelled effects (with uncertainty)?**

In the chapter on your [Starting Model](DSPPH_SM_StartingModel.qmd), you found your coefficients in the "Estimate" column of the `summary()` output of your model: 

```{r echo = FALSE}

forText <- coef(summary(bestMod1)) # extract the coefficients from summary()

```


```{r}

coef(summary(bestMod1)) # extract the coefficients from summary()

```

Interpreting the coefficients from this output takes practice - especially for categorical predictors because of the way that R treats categorical predictors in regression.  With R's "dummy coding", one level of the predictor (here `Sp1`) is incorporated into the intercept estimate (`r round(forText["(Intercept)","Estimate"],1)`) as the reference level.  The other coefficients in the Estimate column represent the change in modelled response when you move from the reference level (`Sp1`) to another level.  

The modelled response when Cat1 = Sp2 is (`r round(forText["Cat1Sp2","Estimate"],1)`)  units higher than this reference level =  `r round(forText["(Intercept)","Estimate"],1)` + `r round(forText["Cat1Sp2","Estimate"],1)` = `r round(forText["Cat1Sp2","Estimate"],1) + round(forText["Cat1Sp2","Estimate"],1)` units.  

The modelled `Resp1` when Cat1 = Sp3 is `r round(forText["Cat1Sp3","Estimate"],1)` units **lower** than the reference level =  `r round(forText["(Intercept)","Estimate"],1)` + `r round(forText["Cat1Sp2","Estimate"],1)` = `r round(forText["Cat1Sp2","Estimate"],1) + round(forText["Cat1Sp3","Estimate"],1)` units.^[note that the p-values reported in the coefficient table are the result of a test of whether the coefficient associated with `Sp2` is different than that of `Sp1` (p = `r round(forText["Cat1Sp2","Pr(>|t|)"],3)`), and if the effect of `Sp3` is different than that of `Sp1` (p = `r round(forText["Cat1Sp3","Pr(>|t|)"],3)`)), but a comparison of effects of `Sp2` vs. `Sp3` is missing.  We will discuss this more further along in these notes.] 

So all the information we need is in this `summary()` output, but not easy to see immediately. An easier way is to use the emmeans^[emmeans stands for "estimated marginal means"] package which helps you by reporting the coefficients directly^[another option is the [marginaleffects package](https://marginaleffects.com/), where marginal effects represent the average effect - the average difference in your predicted response across a change in your categorical predictor level or a unit change in your numeric predictor[@ArelBundockEtAl2024]].  In our case, you use the emmeans package to get the mean value of the response at each level of the predictor.  For categorical predictors, you do this with the `emmeans()` function: 


```{r}

library(emmeans) # load the emmeans package

emmOut <- emmeans(object = bestMod1, # your model
            specs = ~ Cat1, # your predictors
            type = "response") # report coefficients on the response scale

emmOut

```

So now you have a modelled value of your response for each level of your categorical predictor - this captures the effect of your categorical predictor on your response.  You also need to report uncertainty around this effect, and you can do this by reporting the 95% confidence level (CL) also reported by the `emmeans()` function. 

When Cat1 is Sp1, Resp1 is estimated to be `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$emmean[1], 1)` units (95% confidence level: `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$lower.CL[1],1)` to `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$upper.CL[1],1)` units).  
When Cat1 is Sp2, Resp1 is estimated to be `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$emmean[2], 1)` units (95% confidence level: `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$lower.CL[2],1)` to `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$upper.CL[2],1)` units).  
When Cat1 is Sp3, Resp1 is estimated to be `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$emmean[3], 1)` units (95% confidence level: `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$lower.CL[3],1)` to `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$upper.CL[3],1)` units.  

Note that this is the same information in the `summary()` output just easier to read.^[note that the `summary()` output becomes easier to read if you force the starting model not to have an intercept with: `startMod<-glm(formula = Resp1 ~ Cat1 - 1, data = myDat, family = gaussian(link="identity"))`. This is fine to do here where you **only** have categorical predictors but causes problems when you start having continuous predictors in your model as well.  So we won't be practicing this in class and instead will use the very flexible and helpful emmeans package to help us report coefficients from our statistical models.]

Note also that two types of uncertainty are measured here.  `SE` stands for the standard error around the prediction, and is a measure of uncertainty of the average modelled effect.  The `lower.CL` and `upper.CL` represent the 95% confidence limits of the prediction - so if I observed a new `Resp1` at a particular `Cat1`, there would be a 95% chance it would lie between the bounds of the confidence limits.  

Finally, note that you can also get a quick plot of the effects by handing the `emmeans()` output to `plot()`. 

**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

Since you have a categorical predictor with more than two levels, you will need to determine the magnitude and direction of the effect of each level of `Cat1` on `Resp1` - i.e. are the coefficients across factor levels significantly different from one another?

To get evidence about how each level affects your response, you need to test which effects differ from one another among the categories (levels) of your categorical predictor.  This is done using multiple comparisons, i.e. you will compare the modelled effect of each level of your categorical predictor vs. every other level of your categorical predictor to determine which are different from each other (called pairwise testing).  

This is done by testing the null hypothesis that the modelled effects of each pair of category levels are similar to one another, typically rejecting the null hypothesis when P < 0.05. Remember that the P-value is the probability that you observe a value at least as big as the one you observed even though our null hypothesis is true.  In this case, you are looking at the value you are observing as the difference between coefficients estimated for two levels of your predictor.  The P-value is the probability of getting a difference at least as big as the one you observed even though there is actually no difference between the coefficients (the null hypothesis is true).

A couple of things to note about multiple comparison testing:

1) Multiple comparison testing on a categorical predictor should only be done after your hypothesis testing has given you evidence that the predictor has an effect on your response.  That is, you should only do a multiple comparison test on a predictor if that predictor is in your best-specified model.  As this is a test done after your hypothesis testing, it is called a post-hoc^["post hoc" is a Latin phrase meaning "after the event"] test.  

2) Multiple comparison testing can be a problem because you are essentially repeating a hypothesis test many times on the same data (i.e. are the effects of `Sp1` different than those of `Sp2`?  are the effects of `Sp1` different than those of `Sp3`? are the effects of `Sp2` different than those of `Sp3`?...).  These repeated tests mean there is a high chance that you will find a difference in one test **purely due to random chance**, vs. due to there being an actual difference.  The multiple comparison tests you will perform have been formulated to correct for this increased error.

Multiple comparison testing is very simple with the emmeans package.  It just requires you to hand the output from `emmeans()` to a function called `pairs()`:

```{r}

forComp <- pairs(emmOut, # your emmeans object
                 adjust = 'fdr') # multiple comparison adjustment

forComp

```

The output shows the results of the multiple comparison (pairwise) testing.  The values in the p.value column tell you the results of the hypothesis test comparing the coefficients between the two levels of your categorical predictor.  For example, for `Sp1` vs. `Sp2`, there is a `r round(as.data.frame(forComp)$p.value[1],4)*100`% (p = `r round(as.data.frame(forComp)$p.value[1],4)`) probability of getting a difference in coefficients at least as big as `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$emmean[2], 1)` - `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$emmean[1], 1)` = `r `r round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$emmean[2], 1) - round(summary(emmeans(object = bestMod1, specs = ~ Cat1, type = "response"))$emmean[1], 1)` even though the null hypothesis (no difference) is true. This value `r round(as.data.frame(forComp)$p.value[1],4)*100`% (P = `r round(as.data.frame(forComp)$p.value[1],4)`) is small enough that you can say you have evidence that the effect of Cat1 on Resp1 is different for these two different levels (`Sp1` and `Sp2`).

Based on a threshold p-value of 0.05, we can see that:

There is evidence that the value of `Resp1` when `Cat1` is `Sp1` is different (lower) than that when `Cat1` is `Sp2` as P = `r round(as.data.frame(forComp)$p.value[1],4)` is less than P = 0.05.  
There is a little evidence that the value of `Resp1` when `Cat1` is `Sp1` is different (higher) than that when `Cat1` is `Sp3` as P = `r round(as.data.frame(forComp)$p.value[2],4)` is less than P = 0.05 (but it is close!).
There is some evidence that the value of `Resp1` when `Cat1` is `Sp2` is different (higher) than that when `Cat1` is `Sp3` as P < 0.0001^[when the P-value is very low, R reports is as simple <.0001] is smaller than P = 0.05.^[note that now you get to assess the difference between coefficients for `Sp2` and `Sp3` which was missing in the `summary()` output above].

Note that the p-values have been adjusted via the "false discovery rate" (fdr) method [@VerhoevenEtAl2005] which adjusts the difference that the two coefficients need to have to allow for the fact that we are making multiple comparisons.^[The emmeans package is very flexible and has a lot of options as to how to make these corrections depending on your needs. Plenty more information is available 
[here](https://cran.r-project.org/web/packages/emmeans/index.html)]

Note that you can also get the results from the pairwise testing visually by handing the output of `pairs()` to `plot()`.

Note that the difference between `Resp1` when `Cat1` is Sp1 vs. Sp3 is so close to overlapping zero.  This is reflected in the high P value.

:::



:::{.callout-tip collapse="true" title="Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1"} 

###### Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1

Recall that Example 2 contains two predictors and both are categorical:

`Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1`

**What are your modelled effects (with uncertainty)?**

Again, for categorical predictors, there is a coefficient estimated for each level of the predictor.  If there is one or more interactions among predictors, there will be one coefficient for each combination of levels among predictors. Let's look at the `summary()` output of your model to understand better: 

```{r}

coef(summary(bestMod2)) # extract the coefficients from summary()

```
Comparing this output to that of Example 1 shows many more estimated coefficients for Example 2.  This is because we have one coefficient for each level of each predictor, as well as coefficients for each combination (the interaction term) of levels of the predictors.  

Again, it takes a little practice to read the coefficients from the `summary()` output.  For Example 2:

The modelled prediction for `Resp2` when `Cat2` is `TypeA` and `Cat3` is `Treat1` is 365 units (the intercept).  

The modelled prediction for `Resp2` when `Cat2` is `TypeB` and `Cat3` is `Treat1` is 365 + 37 = `r 365+37` units.  

The modelled prediction for `Resp2` when `Cat2` is `TypeA` and `Cat3` is `Treat2` is 365 - 44 = `r 365-44` units.  

The modelled prediction for `Resp2` when `Cat2` is `TypeC` and `Cat3` is `Treat2` is 365 - 76 - 44 + 74 = `r 365 - 76 - 44 + 74` units.

As above, we can use the emmeans package to more easily see these coefficients:

```{r}


emmOut <- emmeans(object = bestMod2, # your model
            specs = ~ Cat2 + Cat3 + Cat2:Cat3, # your predictors
            type = "response") # report coefficients on the response scale

emmOut



```

So now we have a modelled value of our response for each level of our categorical predictor.  For example:  

The modelled prediction for `Resp2` when `Cat2` is `TypeA` and `Cat3` is `Treat1` is 365 units (95% confidence limit: 316-414).

The modelled prediction for `Resp2` when `Cat2` is `TypeB` and `Cat3` is `Treat1` is 402 units (95% confidence limit: 359-446).

The modelled prediction for `Resp2` when `Cat2` is `TypeA` and `Cat3` is `Treat2` is 321 units (95% confidence limit: 268-374).

The modelled prediction for `Resp2` when `Cat2` is `TypeC` and `Cat3` is `Treat2` is 319 units (95% confidence limit: 254-384).  

Finally, note that you can also get a quick plot of the effects by handing the `emmeans()` output to `plot()`.


**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

As with Example 1, you can also find out which combinations of predictor levels are leading to statistically different model predictions in `Resp2`:

```{r}

forComp <- pairs(emmOut, # your emmeans object
                 adjust = 'fdr') # multiple comparison adjustment

forComp

```
This is a hard table to navigate as every possible combination of the levels across predictors is compared.  This may not be what you want.  Instead, you can look for differences of one predictor based on a value of another. For example:

```{r}

forComp <- pairs(emmOut, # emmeans output
                 adjust = 'fdr',  # multiple comparison adjustment
                 simple = "Cat2") # contrasts within this categorical predictor

forComp

```

Here you can more easily see the contrasts, and the adjustment to the P-value will be limited to what you actually need.  Based on a threshold P-value of 0.05, you can see that the effects at some combinations of your predictors are not statistically different from each other.  

For example, the coefficient (predicted `Resp2`) when `Cat3` = `Treat1` and `Cat2` = `TypeA` is 365 and this is 37 lower than the coefficient when `Cat3` = `Treat1` and `Cat2` = `TypeB`, but there is no evidence that the difference has any meaning as P = 0.26 for this comparison.  

On the other hand, some combinations of your predictors are statistically different from each other.  For example, a comparison of modelled `Resp2` when `Cat3` = `Control` and `Cat2` = `TypeB` (548) vs. `Cat3` = `Control` and `Cat2` = `TypeD` (215) shows that they differ by 332 and that there is strong evidence that this difference is meaningful as P < 0.0001.  

Note that you could also organize this with contrasts by `Cat3` instead:

```{r}

forComp <- pairs(emmOut, # emmeans output
                 adjust = 'fdr',  # multiple comparison adjustment
                 simple = "Cat3") # contrasts within this categorical predictor

forComp

```


Note that you can also get the results from the pairwise testing visually by handing the output of `pairs()` to `plot()`.


:::


:::{.callout-tip collapse="true" title="Example 3: Resp3 ~ Cont4 + 1"} 

###### Example 3: Resp3 ~ Cont4 + 1

Recall that Example 3 contains one predictor and the predictor is continuous: 

`Resp3 ~ Cont4 + 1`

**What are your modelled effects (with uncertainty)?**

For a continuous predictor, the effect is captured in one coefficient that describes the change in the response for a unit change in the continuous predictor.  For models with a normal error distribution assumption, this is communicated as the slope.

Let's look at the `summary()` output for Example 3:  

```{r}

coef(summary(bestMod3)) # extract the coefficients from summary()

```

This tells you that for a unit change in `Cont4`, you get a 260 ± 15^[* in units of `Resp3` per units of `Cont4`]. 

Note that this same information is in the emmeans package.  Because `Cont4` is a continuous predictor, you need the `emtrends()` function, which provides the 95% confidence interval on the estimate:

```{r}

trendsOut <- emtrends(bestMod3,  # your model
                      specs = ~ Cont4 + 1, # your predictors
                      var = "Cont4") # your continuous predictors

trendsOut

```
:::{.callout-tip collapse="true" title="Getting a sense for coefficients - normal error distribution assumption"} 

Here's a little thought experiment that will help you get a better sense for your model coefficients.

First, a visual reporting of the modelled effects: 
```{r}

visreg(bestMod3, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cont4", # predictor on x-axis
       #by = ..., # if you want to include a 2nd predictor plotted as colour
       #breaks = ..., # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 3rd predictor
       #overlay = TRUE, # to plot as overlay or panels, when there is 
       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals
       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.
  geom_point(data = myDat3,
             mapping = aes(x = Cont4, y = Resp3))+
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cont4, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

```

If we want to quantitatively report these effects, we are looking for one number that describes how much `Resp3` changes for a unit change in `Cont4`.  We can explore this with the `predict()` function.

First, pick a value of `Cont4` at which to make an estimate of `Resp3`:

```{r}

t1 <- 1 # put any number here


```

The `Resp3` estimated for that `t1` is:



```{r}

## Step One: Define the predictor values for your response prediction
forCont4.1 <- data.frame(Cont4 = t1)

## Step Two: Make your response prediction
myPred.1 <- predict(object = bestMod3, # your model
                  newdata = forCont4.1, # the values of the predictors at which to make the prediction
                  type = "response", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption
                  se.fit = TRUE) # to include a measure of uncertainty around the prediction


myPred.1$fit

```

Now let's find `Resp3` estimated for `t2 = t1 + 1` (i.e. one unit more):

```{r}

t2 <- t1 + 1

## Step One: Define the predictor values for your response prediction
forCont4.2 <- data.frame(Cont4 = t2)

## Step Two: Make your response prediction
myPred.2 <- predict(object = bestMod3, # your model
                  newdata = forCont4.2, # the values of the predictors at which to make the prediction
                  type = "response", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption
                  se.fit = TRUE) # to include a measure of uncertainty around the prediction


myPred.2$fit

```

The effect of `Cont4` on `Resp3` can be expressed as the difference between the two numbers:

```{r}

# Effect of Cont4 on Resp is
myPred.2$fit-myPred.1$fit

```

If you tried this for a number of different t1 values you would notice that the effect is constant.

This slope describes the absolute change in `Resp3.p` from a one unit change in `Cont4`.

Compare this to your modelled coefficients:

```{r}
summary(bestMod3)
```
::: {.alert .alert-info}

So for a **normal error distribution assumption** the effect of a continuous predictor on the response is the **absolute change** in the response for a unit change in the predictor.  And this can be read directly from the model coefficients as the slope of the model fit ($\beta_1$).

:::

:::


**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

This question is only applies to categorical predictors of which there are none in Example 3.

:::


:::{.callout-tip collapse="true" title="Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1"} 

###### Example 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1

Recall that Example 4 contains two predictors, one is categorical and one is continuous:

`Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1`

**What are your modelled effects (with uncertainty)?**

The effects estimated for this model will include values of the response (`Resp4`) at each level of the categorical predictor (`Cat5`) as well as slopes describing the change in the response when the continuous predictor (`Cont6`) changes, and a different slope will be estimated for each level in `Cat5` (this is because of the interaction in the model).  

As above, you can take a look at your modelled coefficients with the `summary()` output: 

```{r}

coef(summary(bestMod4))

```
This shows:

The model prediction of `Resp4` when `Cat5` is `Farm` and `Cont6` is 0 is 98.34 units^[units of `Resp4`] (the intercept).

The model prediction of `Resp4` when `Cat5` is `Urban` and `Cont6` is 0 is 98.34 - 0.24 = `r round(98.34 - 0.24,1)` units.

The model prediction of `Resp4` when `Cat5` is `Wild` and `Cont6` is 0 is 98.34 - 0.87 = `r round(98.34 - 0.87,1)` units.

The **slope** of the relationship between `Cont6` and `Resp4` when `Cat5` is `Farm` is -0.0029.^[units will be the units of `Resp4` divided by those of `Cont6`]

The **slope** of the relationship between `Cont6` and `Resp4` when `Cat5` is `Urban` is -0.0029 + 0.015 = `r -0.0029 + 0.015`.

The **slope** of the relationship between `Cont6` and `Resp4` when `Cat5` is `Wild` is -0.0029 + 0.031 = `r -0.0029 + 0.031`.


Again, interpreting the coefficients from the `summary()` output is tedious and not necessary: You can use the emmeans package to give you the modelled response for each level of the categorical predictor (`Cat5`) directly:

```{r}

emmOut <- emmeans(object = bestMod4, # your model
            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors
            type = "response") # report coefficients on the response scale

emmOut



```
Note that `emmeans()` sets our continuous predictor (`Cont6`) to the mean value of `Cont6` (510 units).  We can also control this with the `at = ` argument:

```{r}



emmOut <- emmeans(object = bestMod4, # your model
            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors
            type = "response", # report coefficients on the response scale
            at = list(Cont6 = 0)) # control the value of your continuous predictor at which to make the coefficient estimates


emmOut


```
By setting `at = 0`, you get the intercept - i.e. the modelled `Resp4` when `Cont6` = 0 for each level of `Cat5`, and this is what is reported in the `summary()` output.


Similarly, you can get the emmeans package to give you the slope coefficients for the continuous predictor (`Cont6`) using the `emtrends()` function, rather than interpreting them from the `summary()` output:

```{r}

trendsOut <- emtrends(bestMod4,  # your model
                      specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors
                      var = "Cont6") # your continuous predictors


trendsOut

```
Note that the 95% confidence limits for the modelled effect for `Cont6` when `Cat5` = `Farm` includes zero.  We can also see this with the `test()` function.

```{r}

test(trendsOut) # get test if coefficient is different than zero.

```
combining the two output, we can report that:

* there is no evidence on an effect on `Resp4` of `Cont6` when `Cat5` = `Farm` (p = 0.44).  

* there is evidence for a positive effect of `Cont6` on `Resp4` when `Cat5` = `Urban`.  The effect is 0.012^[in units of `Resp4` per units of `Cont6`] with a 95% confidence interval of 0.0039 - 0.0205^[in units of `Resp4` per units of `Cont6`].

* there is evidence for a positive effect of `Cont6` on `Resp4` when `Cat5` = `Wild`.  The effect is 0.028^[in units of `Resp4` per units of `Cont6`] with a 95% confidence interval of 0.0182 - 0.0377^[in units of `Resp4` per units of `Cont6`].

**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

<img src="./checkSlopesFirst.png" align="right" width="300px"/>

Since you have a categorical predictor with more than two levels, you will want to report where the effects among levels differ from one another.

Note that the effects in Example 4 include slope coefficients (associated with the continuous predictor) and intercept coefficients (associated with the categorical predictor).  Because the slope and intercept are dependent on one another (when the slope changes, the intercept will naturally change), you need to first check if the slope coefficients vary across your categorical levels.  Only if they **don't** vary (i.e. the modelled effects produce lines that are parallel across your categorical levels) would you go on to test if the intercept coefficiens vary across your categorical levels.  
<br clear="right"/>

You can find out which slopes (i.e. the effect of `Cont6` on `Resp4`) are different across the levels of `Cat5` using `emtrends()` with a request for pairwise testing:

```{r}

forCompSlope <- pairs(trendsOut, # emmeans object
                      adjust = "fdr") # multiple comparison adjustment

forCompSlope

```
Here you see that the slope estimates (the effect of `Cont6` on `Resp4`) for all levels of `Cat5` are significantly different from one another (0.0001 ≤ P ≤ 0.017).

Note that you can also get the results from the pairwise testing visually by handing the output of `pairs()` to `plot()`.

Again: only if all the slopes were similar, would you want to test if the levels of your categorical predictor (`Cat4`) have significantly different coefficients (intercepts) from each other.  You would do this with `pairs()` on the output from `emmeans()` (the `emmOut` object).


:::

---

### Models with a poisson error distribution assumption: {#sec-poissonModelEffects}

**Background**

For models with a poisson error distribution assumption, you will typically start with a link function that is the natural logarithm or `link = "log"`.  You need to take this link function into consideration when you report your modelled effects.

<img src="./poisExampleCoefficients.png" align="right" width="300px"/>

If you are using `link = "log"` (as with a poisson error distribution, and sometimes also a Gamma error distribution - more on this to come), you get your coefficient on the response scale by taking `e` to the coefficient on the link scale (with the R function `exp()`).  This coefficient is called the rate ratio^[also called the incidence rate ratio, incidence density ratio or relative risk] and it tells you the % change in the response for a unit change in your predictor.  

<br clear="right"/>

For a continuous predictor, the effect is captured in one coefficient that describes the change in the response for a unit change in the continuous predictor.  

For models with a normal error distribution assumption, this was simply a slope.  

For models using a log link (including many models with a poisson error distribution assumption), this is communicated as the rate ratio or the % change in the response for a unit change in your predictor.  This allows you to communicate the effect of the continuous predictor while accounting for the curve in the relationship (see visualization of model effects above).  This curve occurs because the model is not allowed to go below zero to be consistent with a poisson error distribution assumption.

:::{.callout-note collapse="true" title="the math"} 

But why do we convert coefficients from the link to response scale with $e^x$^[a reminder that the number `e` that the function `exp()` uses as its base is Euler's number.  It (along with the natural logarithm, `log()`) is used to model exponential growth or decay, and can be used here when you want models where the response can not be below zero.] when `link = "log"`?

A reminder that we can present our linear model like this:

$$
\begin{align}
g(\mu_i|Pred_i) &= \beta_1 \cdot Pred_i + \beta_0 \\
Resp_i &\sim F(\mu_i) 
\end{align}
$$

where 
* $g()$ is the link function
* $\mu_i$ is the mean fitted value $\mu_i$ dependent on $Pred_i$
* $\beta_1$ is the coefficient of $Pred_i$
* $Pred_i$ is the value of your predictor
* $\beta_0$ is the intercept
* $Resp_i$ is the value of the response variable
* $F$ represents some error distribution assumption

If you have an error distribution assumption ($F$), the link function $g()$ is $log_e$^[this is what R uses when you say `link = "log"`]:

$$
\begin{align}
log_e(\mu_i|Pred_i) &= \beta_1 \cdot Pred_i + \beta_0 \\
Resp_i &\sim poisson(\mu_i) 
\end{align}
$$

Then the rate ratio (RR), or % change in the response for a unit change in the predictor becomes,

$$
\begin{align}
RR&=\frac{(\mu_i|Pred = a+1)}{(\mu_i|Pred = a)}\\[2em]
log_e(RR)&=log_e(\frac{(\mu_i|Pred = a+1)}{(\mu_i|Pred = a)})\\[2em]
log_e(RR)&=log_e(\mu_i|Pred = a+1)-log_e(\mu_i|Pred = a)\\[2em]
log_e(RR)&=(\beta_1\cdot (a+1)+\beta_0)-(\beta_1\cdot (a)+\beta_0)\\[2em]
log_e(RR)&=\beta_1\cdot a + \beta_1+\beta_0-\beta_1\cdot a-\beta_0\\[2em]
log_e(RR)&=\beta_1\\[2em]
RR &=e^{\beta_1}
\end{align}
$$

:::


One thing to note: 

The `emmeans()` function **will** convert the coefficients (intercepts) from the link to the response scale.  You can ask for this with the `type = "response"` argument.  

In contrast, the `emtrends()` function **does not** convert the coefficients (slopes) to represent effects on the response scale.  This is because `emtrends()` is reporting the slope of a straight line - the trend line on the link scale.  But the line isn't straight on the response scale.  

We need to convert from the link to the response by hand when we use `emtrends()`^[remember, the coefficients for categorical predictors are also being converted from the link to the response scale.  It is just that R is doing it for us in the `emmeans()` function when we add the argument `type = "response").  How we make the conversion, and interpret the result, will depend on our error distribution assumption: 

<img src="./convertCoef.png" width="700px"/>

Here we will go through examples of reporting for models with a poisson error distribution assumption.  If you want more details on *why* you are using the methods below, check the section on "Models with a normal error distribution assumption" (@sec-normalModelEffects) for context.


**Examples**

Here I have made new examples where the error distribution assumption is poisson.  This changes the response variables, indicated by `Resp1.p`, `Resp2.p`, etc. to be integers ≥ 0.

:::{.callout-tip collapse="true" title="Example 1: Resp1.p ~ Cat1 + 1"} 

###### Example 1: Resp1.p ~ Cat1 + 1

```{r echo = FALSE}

#rm(list=ls())

library(forcats)

n = 50 # sample size

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(312) # for setting random number generating

### Prep predictor(s)
Cat1 <- as.factor(sample(c("StA", "StB", "StC"), n, replace = TRUE))

### generate response
Cat1<-fct_relevel(Cat1, c("StB", "StA", "StC")) # mix up factor level order
forSlope <- runif(1, max = -0.02, min = -0.14) # effect size as change categorical levels (or slope)
forInt <- runif(1, min = 0.5, max = 0.9) # intercept
#### mean response
uResp <- exp((as.numeric(Cat1)*forInt)+2)
  
#### response data
forNoise <- 10 # change vary noise
Resp<- rpois(n,lambda=uResp) + rpois(n, lambda = mean(uResp)*forNoise) 

## make data frame
Cat1<-fct_relevel(Cat1, c("StA", "StB", "StC")) # reorder categorical levels
Dat<-data.frame(Cat1=Cat1, Resp1.p=Resp)
# plot(Resp~Cat1, Dat)

## get best-specified model - normal:
startMod<-glm(formula = Resp1.p ~ Cat1 + 1, # hypothesis
               data = Dat, # data
               family = poisson(link="log")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# 
# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
# 
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values


options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut<-dredge(startMod) # fit and compare a model set representing all possible predictor combinations


bestMod<-(eval(attr(dredgeOut, "model.calls")$`2`)) # extract model # from dredge table


myDat1.p <- Dat
dredgeOut1.p<-dredgeOut
bestMod1.p<-bestMod

```

Recall that Example 1 contains only one predictor and it is categorical:

`Resp1.p ~ Cat1 + 1`

The best-specified model (now with poisson error distribution assumption) is:

```{r}

bestMod1.p

```
that was fit to data in `myDat1.p`:

```{r}

str(myDat1.p)

```

and the `dredge()` table you used to pick your `bestMod1.p` is in `dredgeOut1.p`
```{r}

dredgeOut1.p

```

And here's a visualization of the model effects:

```{r}

# Set up your predictors for the visualized fit
forCat1<-unique(myDat1.p$Cat1) # every value of your categorical predictor

# create a data frame with your predictors
forVis<-expand.grid(Cat1=forCat1) # expand.grid() function makes sure you have all combinations of predictors

# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod1.p, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod1.p)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame



library(ggplot2)

ggplot() + # start ggplot
  geom_point(data = myDat1.p, # add observations to your plot
             mapping = aes(x = Cat1, y = Resp1.p), 
             position=position_jitter(width=0.1)) + # control position of data points so they are easier to see on the plot
  geom_errorbar(data = forVis, # add the uncertainty to your plot
              mapping = aes(x = Cat1, y = Fit, ymin = Lower, ymax = Upper),
              linewidth=1.2) + # control thickness of errorbar line
  geom_point(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cat1, y = Fit), 
             shape = 22, # shape of point
             size = 3, # size of point
             fill = "white", # fill colour for plot
             col = 'black') + # outline colour for plot
  ylab("Resp1.p, (units)")+ # change y-axis label
  xlab("Cat1, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

```

**What are your modelled effects (with uncertainty)?**

For categorical predictors, you can get the coefficients on the response scale directly with the `emmeans()` function^[from the emmeans package] when you specify type = "response": 

```{r}

library(emmeans) # load the emmeans package

emmOut <- emmeans(object = bestMod1.p, # your model
            specs = ~ Cat1 + 1, # your predictors
            type = "response") # report coefficients on the response scale

emmOut

```
Notice that the column reported is "rate": these are the coefficients converted back onto the response scale.  

Note also that two types of uncertainty are measured here.  `SE` stands for the standard error around the prediction, and is a measure of uncertainty of the average modelled effect.  The `lower.CL` and `upper.CL` represent the 95% confidence limits of the prediction - so if I observed a new `Resp1.p` at a particular `Cat1`, there would be a 95% chance it would lie between the bounds of the confidence limits.  

From this output you see: 

When Cat1 is SpA, Resp1.p is estimated to be `r round(summary(emmOut)[1,2])` (95% confidence interval: `r round(summary(emmOut)[1,5])` to `r round(summary(emmOut)[1,6])` units).  
When Cat1 is SpB, Resp1.p is estimated to be `r round(summary(emmOut)[2,2])` (95% confidence interval: `r round(summary(emmOut)[2,5])` to `r round(summary(emmOut)[2,6])` units).
When Cat1 is SpC, Resp1.p is estimated to be `r round(summary(emmOut)[3,2])` (95% confidence interval: `r round(summary(emmOut)[3,5])` to `r round(summary(emmOut)[3,6])` units).

Finally, note that you can also get a quick plot of the effects by handing the `emmeans()` output to `plot()`. 


**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

Here you can compare the effects of the levels of `Cat1` on `Resp1.p` to see if the effects differ between the levels.  

```{r}

forComp <- pairs(emmOut, #emmeans object
                 adjust = "fdr") # multiple testing adjustment

forComp

```

The output shows the results of the multiple comparison (pairwise) testing.  

The values in the `p.value` column tell you the results of the hypothesis test comparing the coefficients between the two levels of your categorical predictor.  

For example: 


* there is evidence that `Resp1.p` is significantly larger when `Cat 1 = SpA` than when `Cat1 = SpB` (p = `r round(as.data.frame(forComp)$p.value[1],4)`).  `Resp1.p` is expected to be ~ `r (round(summary(forComp)[1,2],2)-1)*100` ± `r (round(summary(forComp)[1,3],2))*100`% bigger when `Cat 1 = SpA` than when `Cat 1 = SpB`.

* there is evidence that `Resp1.p` is significantly smaller when `Cat 1 = SpA` than when `Cat1 = SpC` (p = `r round(as.data.frame(forComp)$p.value[2],4)`).  `Resp1.p` is expected to be ~ `r (1-round(summary(forComp)[2,2],2))*100` ± `r (round(summary(forComp)[2,3],2))*100`% smaller when `Cat 1 = SpA` than when `Cat 1 = SpC`.

Note that you can also get the results from the pairwise testing visually by handing the output of `pairs()` to `plot()`.


:::



:::{.callout-tip collapse="true" title="Example 2: Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1"} 

###### Example 2: Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1

```{r echo = FALSE}


n = 50 # sample size

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(913) # for setting random number generating

### Prep predictor(s)
Cat2<-factor(sample(c("TypeA", "TypeB", "TypeC", "TypeD"), size=n, replace=TRUE))
Cat3<-factor(sample(c("Treat", "Control"), size=n, replace=TRUE), levels=c("Treat", "Control"))

### generate response
Cat2<-fct_relevel(Cat2, c("TypeA", "TypeB", "TypeC", "TypeD")) # mix up factor level order
if (ss %% 2 == 0){ Cat3<-fct_relevel(Cat3, c("Treat", "Control")) } # mix up factor level order

forSlope1 <- runif(1, max = -0.9, min = -1.4) # effect size as change categorical levels (or slope)
forSlope2 <- runif(1, min = 0.08, max = 0.14) # effect size as change categorical levels (or slope)
forSlope12 <- 0.4
forInt <- runif(1, min = 2, max = 4) # intercept

#### mean response
uResp <- exp(as.numeric(Cat2)*forSlope1+as.numeric(Cat3)*forSlope2 + as.numeric(Cat2)*as.numeric(Cat3)*forSlope12 + forInt )

#### response data
forNoise <- mean(uResp)/2 # change vary noise
Resp<- rpois(n,lambda=uResp) + rpois(n, lambda = forNoise) 


## make data frame
Cat2<-fct_relevel(Cat2, c("TypeA", "TypeB", "TypeC", "TypeD")) # reorder categorical levels
Cat3<-fct_relevel(Cat3, c("Control", "Treat"))
myDat2.p<-data.frame(Cat2=Cat2, Cat3 = Cat3, Resp2.p=Resp)
# plot(Resp~Cat2, Dat)
# plot(Resp~Cat3, Dat)

## get best-specified model - normal:
startMod<-glm(formula = Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1, # hypothesis
               data = myDat2.p, # data
               family = poisson(link="log")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# 
# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals 
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#            
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat2) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#                  

options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut<-dredge(startMod) # fit and compare a model set representing all possible predictor combinations

bestMod<-(eval(attr(dredgeOut, "model.calls")$`8`))
# 
# visreg(bestMod,
#        xvar = "Cat2",
#        by = "Cat3",
#        overlay = TRUE,
#        scale = "response")


myDat2.p <- myDat2.p
dredgeOut2.p<-dredgeOut
bestMod2.p<-bestMod

```


Recall that Example 2 contains two predictors and both are categorical:

`Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1`

The best-specified model (now with poisson error distribution assumption) is:

```{r}

bestMod2.p

```
that was fit to data in `myDat2.p`:

```{r}

str(myDat2.p)

```

and the dredge() table you used to pick your `bestMod2.p` is in
```{r}

dredgeOut2.p

```

And here's a visualization of the model effects:

```{r}

#### i) choosing the values of your predictors at which to make a prediction

# Set up your predictors for the visualized fit
forCat2<-unique(myDat2.p$Cat2) # every level of your categorical predictor
forCat3<-unique(myDat2.p$Cat3) # every level of your categorical predictor
  
# create a data frame with your predictors
forVis<-expand.grid(Cat2 = forCat2, Cat3 = forCat3) # expand.grid() function makes sure you have all combinations of predictors

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod2.p, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod2.p)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame



#### iii) use the model estimates to plot your model fit

library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  geom_point(data = myDat2.p, # add observations to your plot
             mapping = aes(x = Cat2, y = Resp2.p, col = Cat3), 
             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot
  geom_errorbar(data = forVis, # add the uncertainty to your plot
              mapping = aes(x = Cat2, y = Fit, ymin = Lower, ymax = Upper, col = Cat3),
              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot
              size=1.2) + # control thickness of errorbar line
  geom_point(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cat2, y = Fit, fill = Cat3), 
             shape = 22, # shape of point
             size=3, # size of point
             col = 'black', # outline colour for point
             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot
  ylab("Resp2.p, (units)")+ # change y-axis label
  xlab("Cat2, (units)")+ # change x-axis label
  labs(fill="Cat3, (units)", col="Cat3, (units)") + # change legend title
  theme_bw() # change ggplot theme



```

**What are your modelled effects (with uncertainty)?**

As above, we can use the emmeans package to see the effects of the predictors on the scale of the response:

```{r}


emmOut <- emmeans(object = bestMod2.p, # your model
            specs = ~ Cat2 + Cat3 + Cat2:Cat3, # your predictors
            type = "response") # report coefficients on the response scale

emmOut

```

So now we have a modelled value of our response for each level of our categorical predictors.  For example:  

The modelled prediction for `Resp2.p` when `Cat2` is `TypeA` and `Cat3` is `Treat` is 15.2 (95% confidence interval: 12.1 - 19.0 units).

The modelled prediction for `Resp2.p` when `Cat2` is `TypeC` and `Cat3` is `Control` is 9.7 units  (95% confidence interval: 6.7 - 13.9 units).

Finally, note that you can also get a quick plot of the effects by handing the `emmeans()` output to `plot()`.


**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

As with Example 1, you can also find out which combinations of predictor levels are leading to statistically different model predictions in `Resp2.p`:

```{r}

forComp <- pairs(emmOut, # emmeans output
                 adjust = "fdr", # multiple testing adjustment
                 simple = "Cat2") # contrasts within this categorical predictor

forComp

```

Here you can more easily see the contrasts.  Contrast ratios similar to 1 mean that the predictor levels lead to a similar value of the response. 

Based on a threshold P-value of 0.05, you can see that the effects at some combinations of your predictors are not statistically different from each other.  For example, the coefficient (predicted `Resp2.p`) when `Cat3` = `Control` and `Cat2` = `TypeB` (10.9) is 1.12 times (or 12%) more than the predicted `Resp2.p` when `Cat3` = `Control` and `Cat2` = `TypeC` (9.7), but there is no evidence that the difference has any meaning as P = 0.59 for this comparison.  

On the other hand, some combinations of your predictors are statistically different from each other.  For example, a comparison of modelled `Resp2.p` when `Cat3` = `Control` and `Cat2` = `TypeB` (10.9) is 2.6 times (or 160%) higher than when `Cat3` = `Control` and `Cat2` = `TypeD` (4.2) and there is strong evidence that this difference is meaningful as P < 0.0014.  

Note that you can also get the results from the pairwise testing visually by handing the output of `pairs()` to `plot()`.

:::


:::{.callout-tip collapse="true" title="Example 3: Resp3.p ~ Cont4 + 1"} 

###### Example 3: Resp3.p ~ Cont4 + 1

```{r echo = FALSE}

#rm(list=ls())

n = 50 # sample size

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(994) # for setting random number generating


### Prep predictor(s)
Cont4 <- runif(n, min = 30, max = 120)

### generate response
forSlope2 <- runif(1, min = 0.01, max = 0.03) # effect size as change categorical levels (or slope)
forInt <- runif(1, min = 0.0010, max = 0.0060) # intercept

#### mean response
uResp <- exp(as.numeric(Cont4)*forSlope2 + forInt)

#### response data
forNoise <- mean(uResp)*1 # change vary noise
Resp<- rpois(n,lambda=uResp) + rpois(n, lambda = forNoise) 

## make data frame
myDat3.p<-data.frame(Cont4 = Cont4, Resp3.p=Resp)
# plot(Resp3.p~Cont4, myDat3.p)

## get best-specified model - normal:
startMod<-glm(formula = Resp ~ Cont4 + 1, # hypothesis
               data = myDat3.p, # data
               family = poisson(link="log")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# 
# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals 
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = myDat3.p$Cont4) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#                  

options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut<-dredge(startMod) # fit and compare a model set representing all possible predictor combinations

bestMod<-(eval(attr(dredgeOut, "model.calls")$`2`))

# visreg(bestMod,
#        xvar = "Cont4",
#        scale = "response")


dredgeOut3.p<-dredgeOut
bestMod3.p<-bestMod

```

Recall that Example 3 contains one predictor and the predictor is continuous: 

`Resp3 ~ Cont4 + 1`

The best-specified model (now with poisson error distribution assumption) is:

```{r}

bestMod3.p

```
that was fit to data in `myDat3.p`:

```{r}

str(myDat3.p)

```

and the dredge() table you used to pick your `bestMod3.p` is
```{r}

dredgeOut3.p

```

And here's a visualization of the model effects:

```{r}

#### i) choosing the values of your predictors at which to make a prediction


# Set up your predictors for the visualized fit
forCont4<-seq(from = min(myDat3.p$Cont4), to = max(myDat3.p$Cont4), by = 1)# a sequence of numbers in your continuous predictor range
  
# create a data frame with your predictors
forVis<-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod3.p, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod3.p)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame


#### iii) use the model estimates to plot your model fit


library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  
  geom_point(data = myDat3.p, # add observations to your plot
             mapping = aes(x = Cont4, y = Resp3.p)) + # control position of data points so they are easier to see on the plot
  
  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont4, y = Fit),
              linewidth = 1.2) + # control thickness of line
  
    geom_line(data = forVis, # add uncertainty to your plot (upper line)
              mapping = aes(x = Cont4, y = Upper),
              linewidth = 0.8, # control thickness of line
              linetype = 2) + # control style of line
  
      geom_line(data = forVis, # add uncertainty to your plot (lower line)
              mapping = aes(x = Cont4, y = Lower),
              linewidth = 0.8, # control thickness of line
              linetype = 2) + # control style of line
  
  ylab("Resp3.p, (units)") + # change y-axis label
  
  xlab("Cont4, (units)") + # change x-axis label
  
  theme_bw() # change ggplot theme



```

**What are your modelled effects (with uncertainty)?**

```{r}


trendsOut <- emtrends(bestMod3.p,
                      specs = ~ Cont4 + 1, # your predictors
                      var = "Cont4") # your predictor of interest

trendsOut

```

Note that these are the same estimates as we find in the summary() of your model object

```{r}

coef(summary(bestMod3.p))

```

This is an estimate of the modelled effects in the linked world.  We need to consider our log link function to report the modelled effects in the response world.

Since you have a poisson error distribution assumption, you can convert the estimate made by `emtrends()` on to the response scale with:

```{R}

trendsOut.df <- data.frame(trendsOut) # convert trendsOut to a dataframe

RR <- exp(trendsOut.df$Cont4.trend) # get the coefficient on the response scale

RR

```

We can also use the same conversion on the confidence limits of the modelled effect, for the upper:

```{r}

RR.up <- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale
  
RR.up

```

and lower 95% confidence level:
``` {r}
  
RR.low <- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale

RR.low
  

```


This tells you that for a unit change in `Cont4`, `Resp3` increases by `r round(RR,3)`x (95% confidence interval is `r round(RR.low,3)` to `r round(RR.up,3)`) which is an increase of `r round((RR -1)*100,2)`%. 

:::{.callout-tip collapse="true" title="Getting a sense for coefficients - poisson error distribution assumption"} 

Here's a little thought experiment that will help you get a better sense for your model coefficients when modelling with a poisson error distribution.

First, a visual reporting of the modelled effects: 
```{r}

visreg(bestMod3.p, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cont4", # predictor on x-axis
       #by = ..., # if you want to include a 2nd predictor plotted as colour
       #breaks = ..., # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 3rd predictor
       #overlay = TRUE, # to plot as overlay or panels, when there is 
       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals
       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.
  geom_point(data = myDat3.p,
             mapping = aes(x = Cont4, y = Resp3.p))+
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cont4, (units)")+ # change x-axis label
  ylim(0, 20)+
  theme_bw() # change ggplot theme

```

If we want to quantitatively report these effects, we are looking for one number that describes how much `Resp3` changes for a unit change in `Cont4`.  We can explore this with the `predict()` function.

First, pick a value of `Cont4` at which to make an estimate of `Resp3`:

```{r}

t1 <- 10 # put any number here


```

The `Resp3` estimated for that `t1` is:



```{r}

## Step One: Define the predictor values for your response prediction
forCont4.1 <- data.frame(Cont4 = t1)

## Step Two: Make your response prediction
myPred.1 <- predict(object = bestMod3.p, # your model
                  newdata = forCont4.1, # the values of the predictors at which to make the prediction
                  type = "response", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption
                  se.fit = TRUE) # to include a measure of uncertainty around the prediction


myPred.1$fit

```

Now let's find `Resp3` estimated for `t2 = t1 + 1` (i.e. one unit more):

```{r}

t2 <- t1 + 1

## Step One: Define the predictor values for your response prediction
forCont4.2 <- data.frame(Cont4 = t2)

## Step Two: Make your response prediction
myPred.2 <- predict(object = bestMod3.p, # your model
                  newdata = forCont4.2, # the values of the predictors at which to make the prediction
                  type = "response", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption
                  se.fit = TRUE) # to include a measure of uncertainty around the prediction


myPred.2$fit

```

The effect of `Cont4` on `Resp3` can be expressed as the difference between the two numbers:

```{r}

# Effect of Cont4 on Resp is
myPred.2$fit-myPred.1$fit

```

If your try this for a number of different t1 values you'll notice that the number isn't stable but varies with the values of t1 and t2 we choose.  

Instead, we can express the effect of `Cont4` on `Resp3` as the ratio of the two numbers (called the rate ratio):

```{r}

myPred.2$fit/myPred.1$fit

```

This rate ratio (RR) describes the % change in `Resp3.p` from a one unit change in `Cont4`, and it is stable whatever t1 and t2 values we choose.


Compare this rate ratio to your modelled coefficients:

```{r}
summary(bestMod3.p)
```


```{r}

coef(summary(bestMod3.p))["Cont4", "Estimate"]

```

and note that the rate ratio is the same as `exp(coef)`:

```{r}

exp(coef(summary(bestMod3.p))["Cont4", "Estimate"])

```

::: {.alert .alert-info}

So for a **poisson error distribution assumption** (or Gamma with log link) the effect of a continuous predictor on the response is described as the rate ratio (RR).  This describes the **% change** in the response for a unit change in the predictor.  This is estimated by raising $e$ to the coefficient: $e^{\beta_1}$

:::

:::


**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

This question is only applies to categorical predictors of which their on none in Example 3.


:::


:::{.callout-tip collapse="true" title="Example 4: Resp4.p ~ Cat5 + Cont6 + Cat5:Cont6 + 1"} 

###### Example 4: Resp4.p ~ Cat5 + Cont6 + Cat5:Cont6 + 1

```{r echo = FALSE}

#rm(list=ls())

n = 50 # sample size

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(784) # for setting random number generating

### Prep predictor(s)
Cat5 <- as.factor(sample(c("StA", "StB", "StC"), n, replace = TRUE))
Cont6 <- runif(n, min = 30, max = 120)

### generate response
Cat5<-fct_relevel(Cat5, c("StB", "StA", "StC")) # mix up factor level order
forSlope1 <- runif(1, max = -0.4, min = -0.8) # effect size as change categorical levels (or slope)
forSlope2 <- runif(1, max = 0.04, min = 0.01) # effect size as change categorical levels (or slope)
forSlope12 <- 0.01
forInt <- runif(1, min = 0.1, max = 0.6) # intercept

#### mean response
uResp <- exp(as.numeric(Cat5)*forSlope1+as.numeric(Cont6)*forSlope2 + as.numeric(Cat5)*as.numeric(Cont6)*forSlope12 + forInt)

#### response data
forNoise <- mean(uResp)*6 # change vary noise
Resp<- rpois(n,lambda=uResp) + rpois(n, lambda = forNoise) 

## make data frame
Cat5<-fct_relevel(Cat5, c("StA", "StB", "StC")) # reorder categorical levels
myDat4.p<-data.frame(Cat5=Cat5, Cont6 = Cont6, Resp4.p=Resp)
# plot(Resp~Cat1, Dat)
# plot(Resp~Cont1, Dat)

## get best-specified model - normal:
startMod<-glm(formula = Resp4.p ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # hypothesis
               data = myDat4.p, # data
               family = poisson(link="log")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times

# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals 
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#            
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat2) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#                  

options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut<-dredge(startMod) # fit and compare a model set representing all possible predictor combinations

bestMod<-(eval(attr(dredgeOut, "model.calls")$`8`))

# visreg(bestMod,
#        xvar = "Cont1",
#        by = "Cat1",
#        overlay = TRUE,
#        scale = "response")

dredgeOut4.p <-dredgeOut
bestMod4.p <-bestMod

```


Recall that Example 4 contains two predictors, one is categorical and one is continuous:

`Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1`

The best-specified model (now with poisson error distribution assumption) is:

```{r}

bestMod4.p

```
that was fit to data in `myDat4.p`:

```{r}

str(myDat4.p)

```

and the dredge() table you used to pick your `bestMod4.p` is
```{r}

dredgeOut4.p

```

And here's a visualization of the model effects:

```{r}


#### i) choosing the values of your predictors at which to make a prediction


# Set up your predictors for the visualized fit
forCat5<-unique(myDat4.p$Cat5) # all levels of your categorical predictor
forCont6<-seq(from = min(myDat4.p$Cont6), to = max(myDat4.p$Cont6), by = 1)# a sequence of numbers in your continuous predictor range
  
# create a data frame with your predictors
forVis<-expand.grid(Cat5 = Cat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod4.p, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod4.p)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame


#### iii) use the model estimates to plot your model fit


library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  
  geom_point(data = myDat4.p, # add observations to your plot
             mapping = aes(x = Cont6, y = Resp4.p, col = Cat5)) + # control position of data points so they are easier to see on the plot
  
  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont6, y = Fit, col = Cat5),
              linewidth = 1.2) + # control thickness of line
  
    geom_line(data = forVis, # add uncertainty to your plot (upper line)
              mapping = aes(x = Cont6, y = Upper, col = Cat5),
              linewidth = 0.4, # control thickness of line
              linetype = 2) + # control style of line
  
      geom_line(data = forVis, # add uncertainty to your plot (lower line)
              mapping = aes(x = Cont6, y = Lower, col = Cat5),
              linewidth = 0.4, # control thickness of line
              linetype = 2) + # control style of line
  
  ylab("Resp4, (units)") + # change y-axis label
  
  xlab("Cont6, (units)") + # change x-axis label
  
  labs(fill="Cat5, (units)", col="Cat5, (units)") + # change legend title
  
  theme_bw() # change ggplot theme



```


**What are your modelled effects (with uncertainty)?**

The process for estimating effects of categorical vs. continuous predictors is different.

For categorical predictors (`Cat5`), you can use `emmeans()` to give you the effects on the response scale directly:

```{r}

emmOut <- emmeans(object = bestMod4.p, # your model
            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors
            type = "response") # report coefficients on the response scale

emmOut
```

When Cat5 is StA, Resp4.p is estimated to be `r round(summary(emmOut)[1,2])` (95% confidence interval: `r round(summary(emmOut)[1,5])` to `r round(summary(emmOut)[1,6])` units).  
When Cat5 is StB, Resp4.p is estimated to be `r round(summary(emmOut)[2,2])` (95% confidence interval: `r round(summary(emmOut)[2,5])` to `r round(summary(emmOut)[2,6])` units).
When Cat5 is StC, Resp4.p is estimated to be `r round(summary(emmOut)[3,2])` (95% confidence interval: `r round(summary(emmOut)[3,5])` to `r round(summary(emmOut)[3,6])` units).

For continuous predictors (`Cont6`), you need to use the `emtrends()` function **and then convert the coefficients to the response scale**:

```{r}


trendsOut <- emtrends(bestMod4.p,
                      specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors
                      var = "Cont6") # your predictor of interest

trendsOut


```


Since you have a poisson error distribution assumption, you can convert the estimate made by `emtrends()` on to the response scale with the `exp()` function:

```{R}

trendsOut.df <- data.frame(trendsOut) # convert trendsOut to a dataframe

RR <- exp(trendsOut.df$Cont6.trend) # get the coefficient on the response scale

RR

```

You can also use the same conversion on the confidence limits of the modelled effect, for the upper:

```{r}

RR.up <- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale
  
RR.up

```

and lower 95% confidence level:
``` {r}
  
RR.low <- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale

RR.low
  

```

Let's organize these numbers so we can read the effects easier:

```{r}
RRAll <- data.frame(Cat5 = trendsOut.df$Cat5, # include info on the Cat5 level
                    RR = exp(trendsOut.df$Cont6.trend), # the modelled effect as a rate ratio
                    RR.up = exp(trendsOut.df$asymp.UCL), # upper 95% confidence level of the modelled effect
                    RR.down = exp(trendsOut.df$asymp.LCL)) # lower 95% confidence level of the modelled effect

RRAll

```

This tells you that for a unit change in `Cont6`, `Resp4.p`

* increases by `r round(RRAll[1, "RR"],3)` times (an increase of `r round((RRAll[1, "RR"] -1)*100,2)`%) when `Cat5` is `StA` (95% confidence interval: `r round(RRAll[1, "RR.down"],3)` to `r round(RRAll[1, "RR.up"],3)`).

* increases by `r round(RRAll[2, "RR"],3)` times (an increase of `r round((RRAll[2, "RR"] -1)*100,2)`%) when `Cat5` is `StB` (95% confidence interval: `r round(RRAll[2, "RR.down"],3)` to `r round(RRAll[2, "RR.up"],3)`).

* increases by `r round(RRAll[3, "RR"],3)` times (an increase of `r round((RRAll[3, "RR"] -1)*100,2)`%) when `Cat5` is `StC` (95% confidence interval: `r round(RRAll[3, "RR.down"],3)` to `r round(RRAll[3, "RR.up"],3)`).

Note that the 95% confidence interval for the effect of `Cont6` on `Resp4.p` (the rate ratio) includes 1.  When the rate ratio is 1, there is no effect of the continuous predictor on the response.  See @sec-poissonModelEffects for more explanation on this.  

You can test for evidence that the effects of `Cont6` on `Resp4.p` are significant with: 

```{r}

test(trendsOut) # get test if coefficient is different than zero.

```
Note that these tests are done on the link scale but can be used for reporting effects on the response scale.

From the results, you can see that: 

* there is evidence that there is an effect of `Cont6` on `Resp4.p` when `Cat5` = `StA`. (i.e. the slope on the link scale is different than zero, and the rate ratio on the response scale is different than 1; P < 0.0001).  

* there is *no* evidence that there is an effect of `Cont6` on `Resp4.p` when `Cat5` = `StB`. (i.e. the slope on the link scale is *not* different than zero, and the rate ratio on the response scale is *not* different than 1; P = 0.32).  

* there is evidence that there is an effect of `Cont6` on `Resp4.p` when `Cat5` = `StC`. (i.e. the slope on the link scale is different than zero, and the rate ratio on the response scale is different than 1; P < 0.0001).  

**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

You can also find out which effects of `Cont6` on `Resp4.p` are different from one another using `pairs()` on the `emtrends()` output:

```{r}

forCompRR <- pairs(trendsOut, # emmeans object
                   adjust = "fdr", # multiple comparison adjustment
                   simple = "Cat5") # contrasts within this categorical predictor

forCompRR


```

Remember that you need to convert any predictor coefficients to the response scale when reporting.  

The table above tells you that:

* There is no evidence the effect of `Cont6` on `Resp4.p` differs when `Cat5` is `StA` vs. `StB` (P = 0.12).

* There is evidence that the effect of `Cont6` on `Resp4.p` is different when `Cat5` is `StA` vs. `StC` (P = 0.0022).  The effect of `Cont6` on `Resp4.p` is `exp(-0.0039)` = `r exp(-0.0039)` times as big when `Cat5` is `StA` vs. `StC`.

* There is evidence that the effect of `Cont6` on `Resp4.p` is different when `Cat5` is `StB` vs. `StC` (P = 0.0004).  The effect of `Cont6` on `Resp4.p` is `exp(-0.00652)` = `r exp(-0.00652)` times as big when `Cat5` is `StB` vs. `StC`.

Only if all the slopes were similar: you would want to test if the levels of your categorical predictor (`Cat5`) have significantly different coefficients (intercepts) from each other with `pairs()` on the output from `emmeans()` (the `emmOut` object).


:::

---

### Models with a Gamma error distribution assumption:

**Background**

For models with a Gamma error distribution assumption, you will typically start with a link function that is the inverse or `link = "inverse"`.  You need to take this link function into consideration when you report your modelled effects.

For a model where `link = "inverse"`, the interpretation of the coefficients on the response scale is less clear. In this case, you would use your model to make predictions of your response and report the effects by describing these changes in predictions. We will cover making predictions in the [Predicting section](DSPPH_SM_Predicting.qmd).

**However**, you can also use `link = "log"` for your models with a Gamma error distribution assumption.  Using "log" will let you use the reporting methods in the section on "Models with a poisson error distribution assumption"(@sec-poissonModelEffects).  So, if you have a model with a Gamma error distribution assumption, use `link = "log"` for modelling, and follow the methods given above (@sec-poissonModelEffects).

---

### Models with a binomial error distribution assumption: {#sec-startBinomial}

**Background**

For models with a binomial error distribution assumption, you will typically start with a link function that is the logit function or `link = "logit"`.  You need to take this link function into consideration when you report your modelled effects.

The logit function is also called the "log-odds" function as it is equal to the logarithm of the odds.  

Models with a binomial error distribution assumption model the probability of an event happening.  This might be presence (vs. absence), mature (vs. immature), death (vs. alive), etc., but in all cases this is represented as the probability of your response being 1 (vs. 0).

If $p$ is the probability of an event happening (e.g. response being 1), the probability an event *doesn't* happen is $1-p$ when an event can only have two outcomes. The odds are the probability of the event happening divided by the probability it doesn't happen, or: 

$\frac{p}{1-p}$

The logit function (or log-odds function) is then:

$log_e(\frac{p}{1-p})$

Jargon alert!  Note that a GLM using the logit link function is also called logistic regression.

Alternate link functions for binomially distributed data are probit and cloglog, but the logit link function is by far the one most used as the coefficients that result from the model are the easiest to interpret.  But, as always, you should choose the link function that leads to the best-behaved residuals for your model.
 
Similar to reporting a rate ratio when your link = "log" (@sec-poissonModelEffects), you report your coefficients of a model with a binomial error distribution assumption (and link = "logit") on the response scale by raising `e` to the power of the coefficients estimated on the link scale.  

When you have a **continuous predictor**, $e$ is raised to the slope (estimated on the link scale) to give you an odds ratio (OR) on the response scale.  1 - OR gives you the % change in odds for a unit change in your continuous predictor.  

When you have a **categorical predictor**, $e$ is raised to the intercept of each level of your predictor (estimated on the link scale) to give you the odds associated with that predictor level on the response scale.  

Here is the math linking the coefficient to the odds ratio:

:::{.callout-note collapse="true" title="the math"} 

Given a binomial model, 
$$
\begin{align}
log_e(\frac{p_i}{1-p_i}) &= \beta_1 \cdot Cov_i + \beta_0 \\
Resp_i &\sim binomial(p_i) 
\end{align}
$$
$p_i$ is the probability of success and $1-p_i$ is the probability of failure, and the odds are $\frac{p_i}{1-p_i}$.

Then the odds ratio (OR), or % change in odds due to a change in predictor is:


$$
\begin{align}
OR&=\frac{odds\;when\;Cov=a+1}{odds\;when \;Cov=a}\\[2em]
OR&=\frac{(\frac{p}{1-p}|Cov=a+1)}{(\frac{p}{1-p}|Cov=a)}\\[2em]
log_e(OR)&=(log_e\frac{p}{1-p}|Cov=a+1) - (log_e\frac{p}{1-p}|Cov=a)\\[2em]
log_e(OR)&=(\beta_1\cdot (a+1)+\beta_0) - (\beta_1\cdot a+\beta_0)\\[2em]
log_e(OR)&=\beta_1\cdot a + \beta_1-\beta_1\cdot a\\[2em]
OR &=e^{\beta_1}
\end{align}
$$

:::


One thing to note: 

The `emmeans()` function **will** convert the coefficients (intercepts) from the link to the response scale.  You can ask for this with the `type = "response"` argument.  

In contrast, the `emtrends()` function **does not** convert the coefficients (slopes) to represent effects on the response scale.  This is because `emtrends()` is reporting the slope of a straight line - the trend line on the link scale - but the line isn't straight on the response scale.  

You need to convert from the link to the response by hand when we use `emtrends()`^[remember, the coefficients for categorical predictors are also being converted from the link to the response scale.  It is just that R is doing it for us in the `emmeans()` function when we add the argument `type = "response")].  How you make the conversion, and interpret the result, will depend on your error distribution assumption: 

<img src="./convertCoef.png" width="700px"/>

Here we will go through examples of reporting for models with a binomial error distribution assumption.  If you want more details on *why* you are using the methods below, check the section on "Models with a normal error distribution assumption" (@sec-normalModelEffects) for context.


**Examples**

Here I have made new examples where the error distribution assumption is binomial  This changes the response variables, indicated by `Resp1.b`, `Resp2.b`, etc. to be values of either 0 or 1.

:::{.callout-tip collapse="true" title="Example 1: Resp1.p ~ Cat1 + 1"} 

###### Example 1: Resp1.b ~ Cat1 + 1

```{r echo = FALSE}


n = 50 # sample size

library(forcats)

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(535) # for setting random number generating

### Prep predictor(s)
Cat1 <- as.factor(sample(c("StA", "StB", "StC"), n, replace = TRUE))

### generate response
Cat1<-fct_relevel(Cat1, c("StB", "StA", "StC")) # mix up factor level order
forSlope <- runif(1, max = -0.02, min = -0.14) # effect size as change categorical levels (or slope)
forInt <- runif(1, min = 0.10, max = 0.40) # intercept
#### mean response
forNoise <- 0.4  # change to vary noise (actually controls effect, not noise)
xbeta <- forInt*(as.numeric(Cat1)^(1/forNoise))
xbeta <-xbeta - mean(xbeta)
pr <- 1/(1+exp(-xbeta)) # or plogis(xbeta)?
#### response data
Resp<- rbinom(length(pr), 1, pr)


## make data frame
Cat1<-fct_relevel(Cat1, c("StA", "StB", "StC")) # reorder categorical levels
myDat1<-data.frame(Cat1=Cat1, Resp1.b=Resp)
#plot(Resp1~Cat1, myDat1)

#Dat <- group_by(Dat, Cat1, Resp)


## get best-specified model - normal:
startMod1b<-glm(formula = Resp1.b ~ Cat1 + 1, # hypothesis
               data = myDat1, # data
               family = binomial(link="logit")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod1b, n = 250) # simulate data from our model n times
# 
# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals 
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = myDat1$Cat1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               

options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut1b<-dredge(startMod1b, extra = "R^2") # fit and compare a model set representing all possible predictor combinations

bestMod1.b<-(eval(attr(dredgeOut1b, "model.calls")$`2`))
# library(visreg)
# visreg(bestMod1.b,
#        xvar = "Cat1",
#        scale = "response")


bestMod<-(eval(attr(dredgeOut, "model.calls")$`2`)) # extract model # from dredge table


myDat1.b <- myDat1
dredgeOut1.b<-dredgeOut1b
bestMod1.b<-bestMod1.b

```

Recall that Example 1 contains only one predictor and it is categorical:

`Resp1.b ~ Cat1 + 1`

The best-specified model (now with a binomial error distribution assumption) is:

```{r}

bestMod1.b

```
that was fit to data in `myDat1.b`:

```{r}

str(myDat1.b)

```

and the `dredge()` table you used to pick your `bestMod1.b` is in `dredgeOut1.b`
```{r}

dredgeOut1.b

```

And here's a visualization of the model effects:

```{r}

# Set up your predictors for the visualized fit
forCat1<-unique(myDat1.b$Cat1) # every value of your categorical predictor

# create a data frame with your predictors
forVis<-expand.grid(Cat1=forCat1) # expand.grid() function makes sure you have all combinations of predictors

# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod1.b, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod1.b)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame



library(ggplot2)

ggplot() + # start ggplot
  geom_point(data = myDat1.b, # add observations to your plot
             mapping = aes(x = Cat1, y = Resp1.b)) + 
  geom_errorbar(data = forVis, # add the uncertainty to your plot
              mapping = aes(x = Cat1, y = Fit, ymin = Lower, ymax = Upper),
              linewidth=1.2) + # control thickness of errorbar line
  geom_point(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cat1, y = Fit), 
             shape = 22, # shape of point
             size = 3, # size of point
             fill = "white", # fill colour for plot
             col = 'black') + # outline colour for plot
  ylab("probability Resp1.b = 1")+ # change y-axis label
  xlab("Cat1, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

```

**What are your modelled effects (with uncertainty)?**

For categorical predictors, you can get the coefficients on the response scale directly with the `emmeans()` function^[from the emmeans package] when you specify type = "response".  These are the **probabilities** (not odds or odds ratio) that `Resp1.b` = 1 at each level of your categorical predictor:

```{r}

library(emmeans) # load the emmeans package

emmOut <- emmeans(object = bestMod1.b, # your model
            specs = ~ Cat1, # your predictors
            type = "response") # report coefficients on the response scale

emmOut

```
Notice that the column reported is `prob`: these coefficients are already converted back onto the response scale (as probabilities).  

Note also that two types of uncertainty are measured here.  `SE` stands for the standard error around the prediction, and is a measure of uncertainty of the average modelled effect.  The `lower.CL` and `upper.CL` represent the 95% confidence limits of the prediction - so if you observed a new `Resp1.b` at a particular `Cat1`, there is a 95% probability that the probability `Resp1.b` is 1 would lie within the bounds of the confidence limits.  

From this output you see: 

When `Cat1` is `SpA`, the probability that `Resp1.b` is 1 is estimated to be `r round(summary(emmOut)[1,2],2)` (95% confidence interval: `r round(summary(emmOut)[1,5],2)` to `r round(summary(emmOut)[1,6],2)` units). This is equivalent to odds ($\frac{p}{(1-p)}$) of  `r round((summary(emmOut)[1,2])/(1- summary(emmOut)[1,2]),2)`.

When `Cat1` is `SpB`, the probability that `Resp1.b` is 1 is estimated to be `r round(summary(emmOut)[2,2],2)` (95% confidence interval: `r round(summary(emmOut)[2,5],2)` to `r round(summary(emmOut)[2,6],2)` units). This is equivalent to odds ($\frac{p}{(1-p)}$) of  `r round((summary(emmOut)[2,2])/(1- summary(emmOut)[2,2]),2)`.

When `Cat1` is `SpC`, the probability that `Resp1.b` is 1 is estimated to be `r round(summary(emmOut)[3,2],2)` (95% confidence interval: `r round(summary(emmOut)[3,5],2)` to `r round(summary(emmOut)[3,6],2)` units). This is equivalent to odds ($\frac{p}{(1-p)}$) of  `r round((summary(emmOut)[3,2])/(1- summary(emmOut)[3,2]),2)`.

Finally, note that you can also get a quick plot of the effects by handing the `emmeans()` output to `plot()`. 

:::{.callout-note collapse="true" title="where do these probabilities and odds come from?"} 

The emmeans package provides an easy way of seeing the effects of `Cat1` on `Resp1.b` the response scale, but you can also get this information from the `summary()` output:

```{r}

coef(summary(bestMod1.b))

```
You can convert the coefficients to odds with the `exp()` function, for example: 

When `Cat1` is `SpA`, the **odds** that `Resp1.b` is 1 is estimated to be `exp(coef(summary(bestMod1.b))[1])` =  `r exp(coef(summary(bestMod1.b))[1])`. And you can get the probabilities directly with the `plogis()` function (`plogis(coef(summary(bestMod1.b))[1])` = `r round(plogis(coef(summary(bestMod1.b))[1]),3)`).

:::

**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

Here you can compare the effects of the levels of `Cat1` on `Resp1.b` to see if the effects differ between the levels.  

```{r}

forComp <- pairs(emmOut, # emmeans object
                 adjust = "fdr") # multiple comparison adjustment

forComp

```

The output shows the results of the multiple comparison (pairwise) testing.  

The values in the `p.value` column tell you the results of the hypothesis test comparing the coefficients between the two levels of your categorical predictor.  

Note the `odds.ratio` column here. Comparing effects of `Cat1` on the probability of `Resp1.b` is done by comparing odds of `Resp1.b` being 1 under each level of `Cat1`.

For example: 

* there is no evidence that the probability of `Resp1.b` being 1 is different when `Cat 1 = SpA` vs. when `Cat1 = SpB` (P = `r round(as.data.frame(forComp)$p.value[1],4)`).  

* there is little evidence that the probability of `Resp1.b` being 1 is different when `Cat 1 = SpA` than when `Cat1 = SpC` (P = `r round(as.data.frame(forComp)$p.value[2],4)`).  The odds ratio is `r round(as.data.frame(forComp)$odds.ratio[2],4)`. 

* there is strong evidence that the probability of `Resp1.b` being 1 is different when `Cat 1 = SpB` than when `Cat1 = SpC` (P = `r round(as.data.frame(forComp)$p.value[3],4)`).  The odds ratio is `r round(as.data.frame(forComp)$odds.ratio[3],4)`.

:::{.callout-note collapse="true" title="where did this odds ratio come from?"} 

This is a ratio of the odds of that `Resp1.b` is 1 when `Cat 1 = SpA` vs. the odds of that `Resp1.b` is 1 when `Cat 1 = SpC`:

For example, the odds that `Resp1.b` is 1 when `Cat 1 = SpA` are:
```{r}

pCat1.SpA <- summary(emmOut)[1,2]

oddsCat1.SpA <- pCat1.SpA /(1-pCat1.SpA)

oddsCat1.SpA


```

and the odds that `Resp1.b` is 1 when `Cat 1 = SpC` are:
```{r}

pCat1.SpC <- summary(emmOut)[3,2]

oddsCat1.SpC <- pCat1.SpC /(1-pCat1.SpC)

oddsCat1.SpC


```

and the odds ratio StA/StB is 

```{r}

oddsCat1.SpA/oddsCat1.SpC


```

:::


Note that you can also get the results from the pairwise testing visually by handing the output of `pairs()` to `plot()`.


:::


:::{.callout-tip collapse="true" title="Example 2: Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1"} 

###### Example 2: Resp2.b ~ Cat2 + Cat3 + Cat2:Cat3 + 1

```{r echo = FALSE}


n = 500 # sample size

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(274) # for setting random number generating, 354

### Prep predictor(s)
Cat1 <- as.factor(sample(c("StA", "StB", "StC"), n, replace = TRUE))
Cat2 <- as.factor(sample(c("Control", "Treat"), n, replace = TRUE))

### generate response
Cat1<-fct_relevel(Cat1, c("StB", "StA", "StC")) # mix up factor level order
if (ss %% 2 == 0){ Cat2<-fct_relevel(Cat2, c("Treat", "Control")) } # mix up factor level order

forSlope1 <- runif(1, min=0.1, max = 0.3) # effect size as change categorical levels (or slope)
forSlope2 <- runif(1, min = 0.14, max = 0.18) # effect size as change categorical levels (or slope)
forSlope12 <- 0.052
forInt <- runif(1, min = 0.001, max = 0.004) # intercept


#### mean response
forNoise <-0.1  # change to vary noise (actually controls effect, not noise)
xbeta <- ((as.numeric(Cat1)*forSlope1)+(as.numeric(Cat2)*forSlope2) + (as.numeric(Cat1)*as.numeric(Cat2)*forSlope12))^(1/forNoise)
xbeta <-xbeta - mean(xbeta)
pr <- 1/(1+exp(-xbeta)) # or plogis(xbeta)?
#### response data
Resp<- rbinom(length(pr), 1, pr)


## make data frame
Cat1<-fct_relevel(Cat1, c("StA", "StB", "StC")) # reorder categorical levels
Cat2<-fct_relevel(Cat2, c("Control", "Treat"))
myDat2.b<-data.frame(Cat2=Cat1, Cat3 = Cat2, Resp2.b=Resp)

# 
# Dat <- group_by(Dat, Cat1, Cat2, Resp)
# 
# print(count(Dat))
# 
# 
# plot(Resp~Cat1, Dat)
# plot(Resp~Cat2, Dat)

## get best-specified model - normal:
startMod<-glm(formula = Resp2.b ~ Cat2 + Cat3 + Cat2:Cat3 + 1, # hypothesis
               data = myDat2.b, # data
               family = binomial(link="logit")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# 
# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals 
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#            
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat2) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#                  

options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut<-dredge(startMod, extra = "R^2") # fit and compare a model set representing all possible predictor combinations

bestMod<-(eval(attr(dredgeOut, "model.calls")$`8`))

# visreg(bestMod,
#        xvar = "Cat2",
#        by = "Cat1",
#        overlay = TRUE,
#        scale = "response")



myDat2.b <- myDat2.b
dredgeOut2.b<-dredgeOut
bestMod2.b<-bestMod

```


Recall that Example 2 contains two predictors and both are categorical:

`Resp2.b ~ Cat2 + Cat3 + Cat2:Cat3 + 1`

The best-specified model (now with binomial error distribution assumption) is:

```{r}

bestMod2.b

```
that was fit to data in `myDat2.b`:

```{r}

str(myDat2.b)

```

and the `dredge()` table you used to pick your `bestMod2.b` is in
```{r}

dredgeOut2.b

```

And here's a visualization of the model effects:

```{r}

#### i) choosing the values of your predictors at which to make a prediction

# Set up your predictors for the visualized fit
forCat2<-unique(myDat2.b$Cat2) # every level of your categorical predictor
forCat3<-unique(myDat2.b$Cat3) # every level of your categorical predictor
  
# create a data frame with your predictors
forVis<-expand.grid(Cat2 = forCat2, Cat3 = forCat3) # expand.grid() function makes sure you have all combinations of predictors

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod2.b, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod2.b)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame


#### iii) use the model estimates to plot your model fit

library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  geom_point(data = myDat2.b, # add observations to your plot
             mapping = aes(x = Cat2, y = Resp2.b, col = Cat3), 
             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot
  geom_errorbar(data = forVis, # add the uncertainty to your plot
              mapping = aes(x = Cat2, y = Fit, ymin = Lower, ymax = Upper, col = Cat3),
              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot
              size=1.2) + # control thickness of errorbar line
  geom_point(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cat2, y = Fit, fill = Cat3), 
             shape = 22, # shape of point
             size=3, # size of point
             col = 'black', # outline colour for point
             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot
  ylab("probability Resp2.b = 1")+ # change y-axis label
  xlab("Cat2, (units)")+ # change x-axis label
  labs(fill="Cat3, (units)", col="Cat3, (units)") + # change legend title
  theme_bw() # change ggplot theme



```

**What are your modelled effects (with uncertainty)?**

As above, we can use the emmeans package to see the effects of the predictors on the scale of the response:

```{r}


emmOut <- emmeans(object = bestMod2.b, # your model
            specs = ~ Cat2 + Cat3 + Cat2:Cat3, # your predictors
            type = "response") # report coefficients on the response scale

emmOut

```

So now you have a modelled value of your response for each level of your categorical predictors.  For example:  

When `Cat2` is `StA` and `Cat3` is `Treat`, the probability that `Resp1.b` is 1 is estimated to be `r round(summary(emmOut)[4,3],2)` (95% confidence interval: `r round(summary(emmOut)[4,6],2)` to `r round(summary(emmOut)[4,7],2)` units).  

When `Cat2` is `StB` and `Cat3` is `Control`, the probability that `Resp1.b` is 1 is estimated to be `r round(summary(emmOut)[2,3],2)` (95% confidence interval: `r round(summary(emmOut)[2,6],2)` to `r round(summary(emmOut)[2,7],2)` units).  

Finally, note that you can also get a quick plot of the effects by handing the `emmeans()` output to `plot()`.

**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

As with Example 1, you can also find out which combinations of predictor levels are leading to statistically different model predictions in `Resp2.b`:

```{r}

forComp <- pairs(emmOut, # emmeans output
                 adjust = "fdr", # multiple comparison adjustment
                 simple = "Cat2") # contrasts within this categorical predictor

forComp

```

Based on a threshold P-value of 0.05, you can see that the effects at some combinations of your predictors are not statistically different from each other.  For example, there is **no** evidence that the probability of `Resp2.b` being 1 when `Cat3` = `Control` and `Cat2` = `StA` (`r  round(summary(emmOut)[1,3],2)`)^[taken from the `emmOut` output above] than when `Cat3` = `Control` and `Cat2` = `StB` (`r  round(summary(emmOut)[2,3],2)`) is different from each other (P = `r round(data.frame(forComp)[1,8],2)`).

On the other hand, some combinations of your predictors are statistically different from each other.  For example, there is strong evidence that the probability of `Resp2.b` being 1 when `Cat3` = `Treat` and `Cat2` = `StA` (`r  round(summary(emmOut)[1,3],2)`) is less than that when `Cat3` = `Treat` and `Cat2` = `StC` (`r  round(summary(emmOut)[6,3],2)`) is different from each other (P < 0.0001).

Note that you can also get the results from the pairwise testing visually by handing the output of `pairs()` to `plot()`.

:::


:::{.callout-tip collapse="true" title="Example 3: Resp3.p ~ Cont4 + 1"} 

###### Example 3: Resp3.b ~ Cont4 + 1

```{r echo = FALSE}

n = 300 # sample size

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(100) # for setting random number generating, 308

### Prep predictor(s)
Cont1 <- runif(n, min = 30, max = 120)

forNoise <-1  # change to vary noise (actually controls effect, not noise)
forSlope2 <- runif(1, min = 0.1, max = 0.14) # effect size as change categorical levels (or slope)
forInt <- runif(1, min = 0.001, max = 0.004) # intercept
xbeta <- ((Cont1*forSlope2) + forInt)^(1/forNoise)
xbeta <-xbeta - mean(xbeta)
pr <- 1/(1+exp(-xbeta)) # or plogis(xbeta)?
#### response data
Resp<- rbinom(length(pr), 1, pr)


## make data frame
myDat3.b<-data.frame(Cont4 = Cont1, Resp3.b=Resp)


## get best-specified model - normal:
startMod3.b<-glm(formula = Resp3.b ~ Cont4 + 1, # hypothesis
               data = myDat3.b, # data
               family = binomial(link="logit")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times
# 
# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals 
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               
#            
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = myDat3.b$Cont1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#                  

options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut<-dredge(startMod3.b, extra = "R^2") # fit and compare a model set representing all possible predictor combinations

bestMod<-(eval(attr(dredgeOut, "model.calls")$`2`))




dredgeOut3.b<-dredgeOut
bestMod3.b<-bestMod

```

Recall that Example 3 contains one predictor and the predictor is continuous: 

`Resp3 ~ Cont4 + 1`

The best-specified model (now with a binomial error distribution assumption) is:

```{r}

bestMod3.b

```
that was fit to data in `myDat3.b`:

```{r}

str(myDat3.b)

```

and the `dredge()` table you used to pick your `bestMod3.b` is
```{r}

dredgeOut3.b

```

And here's a visualization of the model effects:

```{r}

#### i) choosing the values of your predictors at which to make a prediction


# Set up your predictors for the visualized fit
forCont4<-seq(from = min(myDat3.b$Cont4), to = max(myDat3.b$Cont4), by = 1)# a sequence of numbers in your continuous predictor range
  
# create a data frame with your predictors
forVis<-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod3.b, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod3.b)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame


#### iii) use the model estimates to plot your model fit


library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  
  geom_point(data = myDat3.b, # add observations to your plot
             mapping = aes(x = Cont4, y = Resp3.b)) + # control position of data points so they are easier to see on the plot
  
  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont4, y = Fit),
              linewidth = 1.2) + # control thickness of line
  
    geom_line(data = forVis, # add uncertainty to your plot (upper line)
              mapping = aes(x = Cont4, y = Upper),
              linewidth = 0.8, # control thickness of line
              linetype = 2) + # control style of line
  
      geom_line(data = forVis, # add uncertainty to your plot (lower line)
              mapping = aes(x = Cont4, y = Lower),
              linetype = 2) + # control style of line
  
  ylab("probability Resp2.b = 1")+ # change y-axis label
  
  xlab("Cont4, (units)") + # change x-axis label
  
  theme_bw() # change ggplot theme



```

The model fit line may seem a little strange as the data are binary (can only be 0 or 1) but the model fit line goes through the space in between 0 and 1.  This model fit line tells you how the probability of `Resp3.b` being 1 changes as `Cont4` changes.  


**What are your modelled effects (with uncertainty)?**

```{r}


trendsOut <- emtrends(bestMod3.b,
                      specs = ~ Cont4 + 1, # your predictors
                      var = "Cont4") # your predictor of interest

trendsOut

```

Note that this is the same estimate as you find in the `summary()` output of your model object:

```{r}

coef(summary(bestMod3.b))

```

This is because `emtrends()` returns coefficients on the *link* scale.  

To report the coefficients on a response scale, you need to consider your link function.  Since you have a binomial error distribution assumption, you can convert the estimate made by `emtrends()` to the response scale with:

```{r}

trendsOut.df <- data.frame(trendsOut) # convert trendsOut to a dataframe

OR <- exp(trendsOut.df$Cont4.trend) # get the coefficient on the response scale

OR

```

You can also use the same conversion on the confidence limits of the modelled effect, for the upper:

```{r}

OR.up <- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale
  
OR.up

```

and lower 95% confidence level:
```{r}
  
OR.low <- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale

OR.low
  

```


This coefficient - called the odds ratio (@sec-startBinomial) - tells you that for a unit change in `Cont4`, the odds that `Resp3.b` is 1 increases by `r round(OR,3)` times (95% confidence interval is `r round(OR.low,3)` to `r round(OR.up,3)`) which is an increase in the odds of `Resp3.b` being 1 of `r round((OR -1)*100,1)`%. 

:::{.callout-note collapse="true" title="where did this odds ratio come from?"} 

You can get a better sense of the odds ratio (vs. odds vs. probabilities) by estimating it directly from the modelled probability that `Resp3.b`  is 1:

Let's find the probability of `Resp3.b` = 1 when `Cont4` is 60: 

```{r}

pCont4.60 <- predict(bestMod3.b, # model
                 newdata = data.frame(Cont4 = 60), # predictor values for the prediction
                 type = "response", # give prediction on the response scale
                 se.fit = TRUE) # include uncertainty

pCont4.60 <- pCont4.60$fit

pCont4.60

```

So there is a `r round(pCont4.60*100,1)`% probability of `Resp3.b` being 1 when `Cont4` is 60.  

The odds associated with `Resp3.b` being 1 when `Cont4` is 60 is given by the probability of `Cont4` is  divided by the probability of absence:

```{r}
oddsCont4.60 <- pCont4.60/(1-pCont4.60)

oddsCont4.60
```

Now let's find the probability of `Resp3.b` being 1 when `Cont4` is 61 (one unit more): 

```{r}
pCont4.61 <- predict(bestMod3.b, # model
                 newdata = data.frame(Cont4 = 61), # predictor values for the prediction
                 type = "response", # give prediction on the response scale
                 se.fit = TRUE) # include uncertainty

pCont4.61 <- pCont4.61$fit

pCont4.61
```

So there is a `r round(pCont4.61*100,1)`% probability of `Resp3.b` being 1 when `Cont4` is 61.  

The odds associated with `Resp3.b` being 1 when `Cont4` is 61 is given by the probability of presence divided by the probability of absence:

```{r}
oddsCont4.61 <- pCont4.61/(1-pCont4.61)

oddsCont4.61
```

Now the odds ratio is found by comparing the change in odds when your predictor (`Cont4`) changes one unit

```{r}
oddsCont4.61/oddsCont4.60
```

Note that this is identical to the calculation of the odds ratio from the coefficient above:

```{r}

OR

```

and you interpret this as a `OR - 1` or `r round((OR-1)*100,1)`% change in the odds of `Resp3.b` being 1 with a unit change of `Cont4`.

:::{.callout-tip collapse="true" title="Getting a sense for coefficients - binomial error distribution assumption"} 

Here's a little thought experiment that will help you get a better sense for your model coefficients.

First, a visual reporting of the modelled effects: 
```{r}

visreg(bestMod3.b, # model to visualize
       scale = "response", # plot on the scale of the response
       xvar = "Cont4", # predictor on x-axis
       #by = ..., # if you want to include a 2nd predictor plotted as colour
       #breaks = ..., # if you want to control how the colour predictor is plotted
       #cond = , # if you want to include a 3rd predictor
       #overlay = TRUE, # to plot as overlay or panels, when there is 
       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals
       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.
  geom_point(data = myDat3.b,
             mapping = aes(x = Cont4, y = Resp3.b))+
  ylab("Response, (units)")+ # change y-axis label
  xlab("Cont4, (units)")+ # change x-axis label
  theme_bw() # change ggplot theme

```

If we want to quantitatively report these effects, we are looking for one number that describes how much `Resp3` changes for a unit change in `Cont4`.  We can explore this with the `predict()` function.

First, pick a value of `Cont4` at which to make an estimate of `Resp3.b`:

```{r}

t1 <- 100 # put any number here


```

The `Resp3.b` estimated for that `t1` is:



```{r}

## Step One: Define the predictor values for your response prediction
forCont4.1 <- data.frame(Cont4 = t1)

## Step Two: Make your response prediction
myPred.1 <- predict(object = bestMod3.b, # your model
                  newdata = forCont4.1, # the values of the predictors at which to make the prediction
                  type = "response", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption
                  se.fit = TRUE) # to include a measure of uncertainty around the prediction


myPred.1$fit

```

Now let's find `Resp3.b` estimated for `t2 = t1 + 1` (i.e. one unit more):

```{r}

t2 <- t1 + 1

## Step One: Define the predictor values for your response prediction
forCont4.2 <- data.frame(Cont4 = t2)

## Step Two: Make your response prediction
myPred.2 <- predict(object = bestMod3.b, # your model
                  newdata = forCont4.2, # the values of the predictors at which to make the prediction
                  type = "response", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption
                  se.fit = TRUE) # to include a measure of uncertainty around the prediction


myPred.2$fit

```

How does `Resp3.b` differ when `Cont4` is t1 vs. t2?

```{r}

myPred.2$fit-myPred.1$fit

```

Try this for a number of different t1 values.  What do you notice?  The number isn't stable but varies with the values of t1 and t2 we choose.  

What about if we express the effect of `Cont4` on the probability of `Resp3.` being 1 as the ratio of the two numbers:

```{r}

myPred.2$fit/myPred.1$fit

```

This number is still not stable.  Instead, we need to compare the **odds** that the `Resp3.b` is 1 when `Cont4` is `t1` vs. `t2`:

```{r}


odds1 <- myPred.1$fit/(1-myPred.1$fit)

odds2 <- myPred.2$fit/(1-myPred.2$fit)

```

and look at the ratio of these odds (called the odds ratio, OR):

```{r}

odds2/odds1

```


This rate ratio (OR) describes the % change in the odds that `Resp3.p` is one when `Cont4` increases one unit.

Compare this odds ratio to your modelled coefficients:

```{r}
summary(bestMod3.b)
```


```{r}

coef(summary(bestMod3.b))["Cont4", "Estimate"]

```

and note that the rate ratio is the same as `exp(coef)`:

```{r}

exp(coef(summary(bestMod3.b))["Cont4", "Estimate"])

```

::: {.alert .alert-info}

So for a **binomial error distribution assumption when using a logit link** the effect of a continuous predictor on the response is described as the odds ratio (OR).  This describes the **% change in the odds that the response is 1** for a unit change in the predictor.  This is estimated by raising $e$ to the coefficient: $e^{\beta_1}$

:::

:::

:::


**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

This question only applies to categorical predictors of which their are none in Example 3.


:::


:::{.callout-tip collapse="true" title="Example 4: Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1"} 

###### Example 4: Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1

```{r echo = FALSE}


n = 500 # sample size

## Prep data
### random number generator tracking
ss<-sample(c(1:1000), 1) # for setting random number generating
set.seed(176) # for setting random number generating, 176

### Prep predictor(s)
Cat1 <- as.factor(sample(c("StA", "StB", "StC"), n, replace = TRUE))
Cont1 <- rnorm(n, 200, 50) #runif(n, min = 30, max = 420)

### generate response
Cat1<-fct_relevel(Cat1, c("StB", "StA", "StC")) # mix up factor level order
forSlope1 <- -12 # effect size as change categorical levels (or slope)
forSlope2 <- 0.030 # effect size as change categorical levels (or slope)
forSlope12 <-0.05
forInt <- 0.4 # effect size as change categorical levels (or slope)
sdNoise <- runif(1, min =0.1, max = 0.3)*0.1


#### mean response
xbeta <- forSlope1*as.numeric(Cat1)+
  forSlope2*Cont1 + 
  forSlope12*(as.numeric(Cat1)*Cont1)+
  forInt+
  rnorm(n, mean = 0, sd = sdNoise)
  
pr <- 1/(1+exp(-xbeta)) # or plogis(xbeta)?

#### response data
Resp<- rbinom(length(pr), 1, pr)
# table(Resp)

## make data frame
Cat1<-fct_relevel(Cat1, c("StA", "StB", "StC")) # reorder categorical levels
myDat4.b<-data.frame(Cat5=Cat1, Cont6 = Cont1, Resp4.b=Resp)

# Dat <- group_by(Dat, Cat1, Resp)
# 
# print(count(Dat))

## get best-specified model - normal:
startMod<-glm(formula = Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # hypothesis
               data = myDat4.b, # data
               family = binomial(link="logit")) # error distribution assumption

simulationOutput <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times

# gg<-testQuantiles(simulationOutput)
# ggResult <-gg$p.value
# 
# ggCont<-testQuantiles(simulationOutput, predictor = Dat$Cont1)
# ggContResult <-ggCont$p.value

# plotQQunif(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#            testUniformity = TRUE, # testing the distribution of the residuals 
#            testOutliers = TRUE, # testing the presence of outliers
#            testDispersion = TRUE) # testing the dispersion of the distribution
# # 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = NULL) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#               
# 
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cat1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#            
# plotResiduals(simulationOutput, # the object made when estimating the scaled residuals.  See section 2.1 above
#               form = Dat$Cont1) # the variable against which to plot the residuals.  When form = NULL, we see the residuals vs. fitted values
#                  

options(na.action = "na.fail") # needed for dredge() function to prevent illegal model comparisons

dredgeOut<-dredge(startMod, extra = "R^2") # fit and compare a model set representing all possible predictor combinations

bestMod<-get.models(dredgeOut, subset = 1)[[1]]
# 
# visreg(bestMod,
#        xvar = "Cont1",
#        by = "Cat1",
#        overlay = TRUE,
#        scale = "response")
# 
# library(emmeans)
# 
# trendsOut <- emtrends(bestMod,
#                       specs = ~ Cat1 + Cont1 + Cat1:Cont1 + 1 , # your predictors
#                       var = "Cont1") # your predictor of interest
# 
# 
# slopeComp <- pairs(trendsOut)


dredgeOut4.b <-dredgeOut
bestMod4.b <-bestMod

```


Recall that Example 4 contains two predictors, one is categorical and one is continuous:

`Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1`

The best-specified model (now with a binomial error distribution assumption) is:

```{r}

bestMod4.b

```

that was fit to data in `myDat4.b`:

```{r}

str(myDat4.b)

```

and the `dredge()` table you used to pick your `bestMod4.b` is
```{r}

dredgeOut4.b

```

And here's a visualization of the model effects:

```{r}


#### i) choosing the values of your predictors at which to make a prediction


# Set up your predictors for the visualized fit
forCat5<-unique(myDat4.b$Cat5) # all levels of your categorical predictor
forCont6<-seq(from = min(myDat4.b$Cont6), to = max(myDat4.b$Cont6), by = 1)# a sequence of numbers in your continuous predictor range
  
# create a data frame with your predictors
forVis<-expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  

#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor


# Get your model fit estimates at each value of your predictors
modFit<-predict(bestMod4.b, # the model
                newdata = forVis, # the predictor values
                type = "link", # here, make the predictions on the link scale and we'll translate the back below
                se.fit = TRUE) # include uncertainty estimate

ilink <- family(bestMod4.b)$linkinv # get the conversion (inverse link) from the model to translate back to the response scale

forVis$Fit<-ilink(modFit$fit) # add your fit to the data frame
forVis$Upper<-ilink(modFit$fit + 1.96*modFit$se.fit) # add your uncertainty to the data frame, 95% confidence intervals
forVis$Lower<-ilink(modFit$fit - 1.96*modFit$se.fit) # add your uncertainty to the data frame


#### iii) use the model estimates to plot your model fit


library(ggplot2) # load ggplot2 library

ggplot() + # start ggplot
  
  geom_point(data = myDat4.b, # add observations to your plot
             mapping = aes(x = Cont6, y = Resp4.b, col = Cat5)) + # control position of data points so they are easier to see on the plot
  
  geom_line(data = forVis, # add the modelled fit to your plot
              mapping = aes(x = Cont6, y = Fit, col = Cat5),
              linewidth = 1.2) + # control thickness of line
  
    geom_line(data = forVis, # add uncertainty to your plot (upper line)
              mapping = aes(x = Cont6, y = Upper, col = Cat5),
              linewidth = 0.4, # control thickness of line
              linetype = 2) + # control style of line
  
      geom_line(data = forVis, # add uncertainty to your plot (lower line)
              mapping = aes(x = Cont6, y = Lower, col = Cat5),
              linewidth = 0.4, # control thickness of line
              linetype = 2) + # control style of line
  
  ylab("probability Resp2.b = 1")+ # change y-axis label
  
  xlab("Cont6, (units)") + # change x-axis label
  
  labs(fill="Cat5, (units)", col="Cat5, (units)") + # change legend title
  
  theme_bw() # change ggplot theme



```


**What are your modelled effects (with uncertainty)?**

The process for estimating effects of categorical vs. continuous predictors is different.

For categorical predictors (`Cat5`), you can use `emmeans()` to give you the effects on the response scale directly:

```{r}

emmOut <- emmeans(object = bestMod4.b, # your model
            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors
            type = "response") # report coefficients on the response scale

emmOut
```

This output tells you that:

* When `Cat5` is StA and `Cont6` is 227 units^[the mean of the range of `Cont6`], there is a `r round(summary(emmOut)[1,3]*100)`% probability that `Resp4.b` will be 1 (95% confidence interval: `r round(summary(emmOut)[1,6],2)` to `r round(summary(emmOut)[1,7],2)`%) 

* When `Cat5` is StB and `Cont6` is 227 units^[the mean of the range of `Cont6`], there is a `r round(summary(emmOut)[2,3]*100)`% probability that `Resp4.b` will be 1 (95% confidence interval: `r round(summary(emmOut)[2,6],2)` to `r round(summary(emmOut)[2,7],2)` units).

* When `Cat5` is StB and `Cont6` is 227 units^[the mean of the range of `Cont6`], there is a `r round(summary(emmOut)[3,3]*100)`% probability that `Resp4.b` will be 1 (95% confidence interval: `r round(summary(emmOut)[3,6],2)` to `r round(summary(emmOut)[3,7],2)` units).

Note that `emmeans()` sets your continuous predictor (`Cont6`) to the mean value of `Cont6` (227 units).  You can also control this with the `at = ` argument in the `emmeans()` function:

```{r}

emmOut <- emmeans(object = bestMod4.b, # your model
            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors
            type = "response", # report coefficients on the response scale
            at = list(Cont6 = 150)) # control the value of your continuous predictor at which to make the coefficient estimates


emmOut


```


For continuous predictors (`Cont6`), you need to use the `emtrends()` function **and then convert the coefficients to the response scale**:

```{r}


trendsOut <- emtrends(bestMod4.b,
                      specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1 , # your predictors
                      var = "Cont6") # your predictor of interest

trendsOut



```

Since you have a binomial error distribution assumption, you can convert the estimate made by `emtrends()` on to the response scale with the `exp()` function:

```{R}

trendsOut.df <- data.frame(trendsOut) # convert trendsOut to a dataframe

OR <- exp(trendsOut.df$Cont6.trend) # get the coefficient on the response scale

OR

```

You can also use the same conversion on the confidence limits of the modelled effect, for the upper:

```{r}

OR.up <- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale
  
OR.up

```

and lower 95% confidence level:
```{r}
  
OR.low <- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale

OR.low
  

```

Let's organize these numbers so we can read the effects easier:

```{r}
ORAll <- data.frame(Cat5 = trendsOut.df$Cat5, # include info on the Cat5 level
                    OR = exp(trendsOut.df$Cont6.trend), # the modelled effect as a rate ratio
                    OR.up = exp(trendsOut.df$asymp.UCL), # upper 95% confidence level of the modelled effect
                    OR.down = exp(trendsOut.df$asymp.LCL)) # lower 95% confidence level of the modelled effect

ORAll

```

This tells you that for a unit change in `Cont6`, the odds that `Resp4.b` is 1:

* increases by `r (round(ORAll[1, "OR"],3)-1)*100`% with a unit change in `Cont6` when `Cat5` is `StA` (95% confidence interval: `r (round(ORAll[1, "OR.down"],3)-1)*100` to `r (round(ORAll[1, "OR.up"],3)-1)*100`).

* increases by `r (round(ORAll[2, "OR"],3)-1)*100`% with a unit change in `Cont6` when `Cat5` is `StB` (95% confidence interval: `r (round(ORAll[2, "OR.down"],3)-1)*100` to `r (round(ORAll[2, "OR.up"],3)-1)*100`).

* increases by `r (round(ORAll[3, "OR"],3)-1)*100`% with a unit change in `Cont6` when `Cat5` is `StC` (95% confidence interval: `r (round(ORAll[3, "OR.down"],3)-1)*100` to `r (round(ORAll[3, "OR.up"],3)-1)*100`).


You can test for evidence that the effects of `Cont6` on `Resp4.b` are significant (different than zero) with: 

```{r}

test(trendsOut) # get test if coefficient is different than zero.

```
Note that these tests are done on the link scale but can be used for reporting effects on the response scale.

From the results, you can see that the effects of `Cont6` on `Resp4.b` at all levels of `Cat5` are significant (P < 0.0001).

**Are the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)**

You can also find out which effects of `Cont6` on `Resp4.b` are different from one another using `pairs()` on the `emtrends()` output:

```{r}

forCompOR <- pairs(trendsOut, # emmeans object
                   adjust = "fdr",  # multiple comparison adjustment
                   simple = "Cat5") # contrasts within this categorical predictor

forCompOR


```

The table above tells you that:

* There is evidence that the effect of `Cont6` on `Resp4.b` is different when `Cat5` is `StA` vs. `StB` (P = `r round(as.data.frame(forCompOR)$p.value[1],3)`).  The effect of `Cont6` on `Resp4.b` is `exp(0.0954)` = `r round(exp(as.data.frame(forCompOR)$estimate[1]),2)` times as big when `Cat5` is `StA` vs. `StB` (i.e. the effect of `Cont6` on `Resp4.b` is `r round((exp(0.0954)-1)*100)`% more when `Cat5` is `StA` vs. `StB`)

* There is no evidence the effect of `Cont6` on `Resp4.b` differs when `Cat5` is `StA` vs. `StC` (P = `r round(as.data.frame(forCompOR)$p.value[2],3)`).

* There is evidence that the effect of `Cont6` on `Resp4.b` is different when `Cat5` is `StB` vs. `StC` (P = `r round(as.data.frame(forCompOR)$p.value[3],3)`).  The effect of `Cont6` on `Resp4.b` is `exp(-0.1317)` = `r round(exp(-0.1317),2)` times as big when `Cat5` is `StB` vs. `StC` (i.e. the effect of `Cont6` on `Resp4.b` is `r round((1-exp(-0.1317))*100)`% less when `Cat5` is `StB` vs. `StC`)

Only if all the slopes^[i.e. effects associated with the continuous predictor] were similar, you would want to test if the levels of your categorical predictor (`Cat5`) have significantly different coefficients (intercepts) from each other with `pairs()` on the output from `emmeans()` (the `emmOut` object).

:::{.callout-caution collapse="true" title="Aside: how does this relate to the `summary()` output?"} 

:::

:::

# Up next {.unnumbered}

In the next chapter, we will discuss how you can use your model to make [predictions](DSPPH_SM_Predicting.qmd). 



